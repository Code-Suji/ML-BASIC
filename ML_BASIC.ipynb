{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_BASIC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Code-Suji/ML-BASIC/blob/master/ML_BASIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1FWg4De3h0m"
      },
      "source": [
        "**Simple Iris Classification Problem**\n",
        "\n",
        "In this case, we will try to classify Iris Dataset into three classes (Iris Setosa, Iris Virginica, and Iris Versicolor) based on four attributes: Sepal Length, Sepal Width, Petal Length, and Petal Width.\n",
        "\n",
        "Iris dataset description can be found in http://archive.ics.uci.edu/ml/datasets/iris\n",
        "\n",
        "Iris data can be downloaded from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "Image for post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf5rXAXt3wdo"
      },
      "source": [
        "**Data Preparation**\n",
        "\n",
        "First, we must import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC1wH4C933qh"
      },
      "source": [
        "import pandas as pd #Python Data Analysis Library \n",
        "import numpy as np #Python Scientific Library "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM5jCozT3-aH"
      },
      "source": [
        "By using pandas, we can download dataset from a given URL and convert it into a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jKQKsWq4FYa",
        "outputId": "36515800-d408-4b2a-cc90-0458e053a379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length','sepal_width','petal_length','petal_width','iris_class']\n",
        "dataset = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')\n",
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   iris_class    150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pas711Ld4S8d"
      },
      "source": [
        "Letâ€™s analyze our dataset. Use dataset.head(n) to display top n data. Change dataset.head(n) to dataset.sample(n) to display random data.\n",
        "Image for post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMiNDUA14mSA",
        "outputId": "1855ef45-c949-4ff1-fbae-851942630100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "dataset.head(6)\n",
        "  #change head with sample and tail"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width   iris_class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
              "5           5.4          3.9           1.7          0.4  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3by7jc945UU"
      },
      "source": [
        "Now that our dataset is ready we can separate input features (x) and target class (y). Input feature will be 150x4 matrix (150 data x sepal_length, sepal_width, petal_length, and petal_width) and target output 150x1 (iris_class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhhQ6PHW4_e0",
        "outputId": "3ae24979-2807-41a7-fd30-fa865ca514a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893gF5Ok5F-e"
      },
      "source": [
        "In this session, we will use a Multi-Layer Perceptron (MLP) Classifier. We need to encode our target attribute for Neural Network based classifier into one hot format. We can do this by calling the Pandas method get_dummies(y). With this method, we will convert:\n",
        "Iris setosa: 100\n",
        "Iris versicolor: 010\n",
        "Iris virginica: 001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSViWhmC5KS1",
        "outputId": "a9798487-e3ea-4962-d4e7-c57a8bab82d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "#one hot encoding\n",
        "y=pd.get_dummies(y)\n",
        "y.sample(7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "126            0                0               1\n",
              "48             1                0               0\n",
              "73             0                1               0\n",
              "52             0                1               0\n",
              "57             0                1               0\n",
              "36             1                0               0\n",
              "130            0                0               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7O4ShTn5Qvn"
      },
      "source": [
        "Now that our input and target are ready, we can separate our training and testing set by using scikit learn method train_test_split()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141o8_Np5R6P",
        "outputId": "9d66431e-5933-4fae-cbe3-d3fbbec7a499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Selective import Scikit Learn \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(105, 4) (105, 3)\n",
            "(45, 4) (45, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GqxCQxw5lKZ"
      },
      "source": [
        "You can try a smaller or bigger test set by changing the test_size parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjkwl8w5nA1"
      },
      "source": [
        "The machine learning part\n",
        "\n",
        "For our machine learning, we will use the sklearn implementation of Multi-Layer Perceptron (A neural network architecture): https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html.\n",
        "\n",
        "Our neural-net will be 4 x 10 x 5 x 3 so hidden layers are (10 units and 5 units).\n",
        "Set our max iteration to 2000 to train for 2000 epoch, and alpha to 0.01 to set our learning rate.\n",
        "Set verbose to 1 to log your training process.\n",
        "Random_state is used as a random seed so we can get the same output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNDnyIv5rzB",
        "outputId": "3e55ffd0-c970-448a-8a2c-b2a06e3174eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Importing our model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.52195752\n",
            "Iteration 2, loss = 2.48553001\n",
            "Iteration 3, loss = 2.43773168\n",
            "Iteration 4, loss = 2.38267019\n",
            "Iteration 5, loss = 2.32543764\n",
            "Iteration 6, loss = 2.26929853\n",
            "Iteration 7, loss = 2.21546796\n",
            "Iteration 8, loss = 2.16450940\n",
            "Iteration 9, loss = 2.11754593\n",
            "Iteration 10, loss = 2.07503576\n",
            "Iteration 11, loss = 2.03721718\n",
            "Iteration 12, loss = 2.00446226\n",
            "Iteration 13, loss = 1.97702861\n",
            "Iteration 14, loss = 1.95479032\n",
            "Iteration 15, loss = 1.93723974\n",
            "Iteration 16, loss = 1.92452673\n",
            "Iteration 17, loss = 1.91526472\n",
            "Iteration 18, loss = 1.90834108\n",
            "Iteration 19, loss = 1.90328167\n",
            "Iteration 20, loss = 1.89928192\n",
            "Iteration 21, loss = 1.89565881\n",
            "Iteration 22, loss = 1.89208023\n",
            "Iteration 23, loss = 1.88847980\n",
            "Iteration 24, loss = 1.88469312\n",
            "Iteration 25, loss = 1.88071867\n",
            "Iteration 26, loss = 1.87654433\n",
            "Iteration 27, loss = 1.87215645\n",
            "Iteration 28, loss = 1.86772937\n",
            "Iteration 29, loss = 1.86323296\n",
            "Iteration 30, loss = 1.85865133\n",
            "Iteration 31, loss = 1.85406468\n",
            "Iteration 32, loss = 1.84941361\n",
            "Iteration 33, loss = 1.84472765\n",
            "Iteration 34, loss = 1.84003883\n",
            "Iteration 35, loss = 1.83536785\n",
            "Iteration 36, loss = 1.83073211\n",
            "Iteration 37, loss = 1.82614166\n",
            "Iteration 38, loss = 1.82161028\n",
            "Iteration 39, loss = 1.81713557\n",
            "Iteration 40, loss = 1.81271457\n",
            "Iteration 41, loss = 1.80834348\n",
            "Iteration 42, loss = 1.80402096\n",
            "Iteration 43, loss = 1.79974599\n",
            "Iteration 44, loss = 1.79551753\n",
            "Iteration 45, loss = 1.79137921\n",
            "Iteration 46, loss = 1.78753191\n",
            "Iteration 47, loss = 1.78391874\n",
            "Iteration 48, loss = 1.78047154\n",
            "Iteration 49, loss = 1.77715339\n",
            "Iteration 50, loss = 1.77387602\n",
            "Iteration 51, loss = 1.77080306\n",
            "Iteration 52, loss = 1.76776995\n",
            "Iteration 53, loss = 1.76470915\n",
            "Iteration 54, loss = 1.76163352\n",
            "Iteration 55, loss = 1.75850857\n",
            "Iteration 56, loss = 1.75544004\n",
            "Iteration 57, loss = 1.75243678\n",
            "Iteration 58, loss = 1.74946370\n",
            "Iteration 59, loss = 1.74662616\n",
            "Iteration 60, loss = 1.74382067\n",
            "Iteration 61, loss = 1.74105859\n",
            "Iteration 62, loss = 1.73830086\n",
            "Iteration 63, loss = 1.73553032\n",
            "Iteration 64, loss = 1.73275457\n",
            "Iteration 65, loss = 1.72998768\n",
            "Iteration 66, loss = 1.72721050\n",
            "Iteration 67, loss = 1.72442128\n",
            "Iteration 68, loss = 1.72166782\n",
            "Iteration 69, loss = 1.71892333\n",
            "Iteration 70, loss = 1.71616641\n",
            "Iteration 71, loss = 1.71339064\n",
            "Iteration 72, loss = 1.71060573\n",
            "Iteration 73, loss = 1.70780763\n",
            "Iteration 74, loss = 1.70499810\n",
            "Iteration 75, loss = 1.70220192\n",
            "Iteration 76, loss = 1.69939212\n",
            "Iteration 77, loss = 1.69656976\n",
            "Iteration 78, loss = 1.69373373\n",
            "Iteration 79, loss = 1.69088431\n",
            "Iteration 80, loss = 1.68803068\n",
            "Iteration 81, loss = 1.68516444\n",
            "Iteration 82, loss = 1.68227922\n",
            "Iteration 83, loss = 1.67938091\n",
            "Iteration 84, loss = 1.67647537\n",
            "Iteration 85, loss = 1.67355718\n",
            "Iteration 86, loss = 1.67062367\n",
            "Iteration 87, loss = 1.66767517\n",
            "Iteration 88, loss = 1.66469632\n",
            "Iteration 89, loss = 1.66169949\n",
            "Iteration 90, loss = 1.65867960\n",
            "Iteration 91, loss = 1.65563162\n",
            "Iteration 92, loss = 1.65255479\n",
            "Iteration 93, loss = 1.64944839\n",
            "Iteration 94, loss = 1.64632380\n",
            "Iteration 95, loss = 1.64317473\n",
            "Iteration 96, loss = 1.63999805\n",
            "Iteration 97, loss = 1.63679461\n",
            "Iteration 98, loss = 1.63356787\n",
            "Iteration 99, loss = 1.63031070\n",
            "Iteration 100, loss = 1.62702739\n",
            "Iteration 101, loss = 1.62371583\n",
            "Iteration 102, loss = 1.62038081\n",
            "Iteration 103, loss = 1.61701502\n",
            "Iteration 104, loss = 1.61361807\n",
            "Iteration 105, loss = 1.61018843\n",
            "Iteration 106, loss = 1.60672504\n",
            "Iteration 107, loss = 1.60323054\n",
            "Iteration 108, loss = 1.59970473\n",
            "Iteration 109, loss = 1.59614272\n",
            "Iteration 110, loss = 1.59255216\n",
            "Iteration 111, loss = 1.58895033\n",
            "Iteration 112, loss = 1.58534466\n",
            "Iteration 113, loss = 1.58172315\n",
            "Iteration 114, loss = 1.57806635\n",
            "Iteration 115, loss = 1.57438205\n",
            "Iteration 116, loss = 1.57069242\n",
            "Iteration 117, loss = 1.56698155\n",
            "Iteration 118, loss = 1.56324826\n",
            "Iteration 119, loss = 1.55949474\n",
            "Iteration 120, loss = 1.55574018\n",
            "Iteration 121, loss = 1.55197763\n",
            "Iteration 122, loss = 1.54820322\n",
            "Iteration 123, loss = 1.54443403\n",
            "Iteration 124, loss = 1.54067431\n",
            "Iteration 125, loss = 1.53689693\n",
            "Iteration 126, loss = 1.53310203\n",
            "Iteration 127, loss = 1.52929011\n",
            "Iteration 128, loss = 1.52547774\n",
            "Iteration 129, loss = 1.52166771\n",
            "Iteration 130, loss = 1.51791472\n",
            "Iteration 131, loss = 1.51413009\n",
            "Iteration 132, loss = 1.51032664\n",
            "Iteration 133, loss = 1.50654388\n",
            "Iteration 134, loss = 1.50274612\n",
            "Iteration 135, loss = 1.49896121\n",
            "Iteration 136, loss = 1.49516951\n",
            "Iteration 137, loss = 1.49139001\n",
            "Iteration 138, loss = 1.48761218\n",
            "Iteration 139, loss = 1.48382926\n",
            "Iteration 140, loss = 1.48003868\n",
            "Iteration 141, loss = 1.47624031\n",
            "Iteration 142, loss = 1.47244114\n",
            "Iteration 143, loss = 1.46863635\n",
            "Iteration 144, loss = 1.46482448\n",
            "Iteration 145, loss = 1.46100507\n",
            "Iteration 146, loss = 1.45718423\n",
            "Iteration 147, loss = 1.45335275\n",
            "Iteration 148, loss = 1.44951568\n",
            "Iteration 149, loss = 1.44568179\n",
            "Iteration 150, loss = 1.44184159\n",
            "Iteration 151, loss = 1.43799499\n",
            "Iteration 152, loss = 1.43413956\n",
            "Iteration 153, loss = 1.43027879\n",
            "Iteration 154, loss = 1.42640937\n",
            "Iteration 155, loss = 1.42253876\n",
            "Iteration 156, loss = 1.41865363\n",
            "Iteration 157, loss = 1.41476927\n",
            "Iteration 158, loss = 1.41087345\n",
            "Iteration 159, loss = 1.40697288\n",
            "Iteration 160, loss = 1.40307194\n",
            "Iteration 161, loss = 1.39916202\n",
            "Iteration 162, loss = 1.39525080\n",
            "Iteration 163, loss = 1.39133409\n",
            "Iteration 164, loss = 1.38742270\n",
            "Iteration 165, loss = 1.38349834\n",
            "Iteration 166, loss = 1.37957841\n",
            "Iteration 167, loss = 1.37565558\n",
            "Iteration 168, loss = 1.37172998\n",
            "Iteration 169, loss = 1.36780219\n",
            "Iteration 170, loss = 1.36387390\n",
            "Iteration 171, loss = 1.35994523\n",
            "Iteration 172, loss = 1.35601931\n",
            "Iteration 173, loss = 1.35209038\n",
            "Iteration 174, loss = 1.34816303\n",
            "Iteration 175, loss = 1.34423615\n",
            "Iteration 176, loss = 1.34031399\n",
            "Iteration 177, loss = 1.33638993\n",
            "Iteration 178, loss = 1.33246904\n",
            "Iteration 179, loss = 1.32855089\n",
            "Iteration 180, loss = 1.32463545\n",
            "Iteration 181, loss = 1.32072408\n",
            "Iteration 182, loss = 1.31681866\n",
            "Iteration 183, loss = 1.31291545\n",
            "Iteration 184, loss = 1.30901643\n",
            "Iteration 185, loss = 1.30512239\n",
            "Iteration 186, loss = 1.30123389\n",
            "Iteration 187, loss = 1.29735131\n",
            "Iteration 188, loss = 1.29347907\n",
            "Iteration 189, loss = 1.28961747\n",
            "Iteration 190, loss = 1.28574983\n",
            "Iteration 191, loss = 1.28189396\n",
            "Iteration 192, loss = 1.27805474\n",
            "Iteration 193, loss = 1.27421298\n",
            "Iteration 194, loss = 1.27037960\n",
            "Iteration 195, loss = 1.26656203\n",
            "Iteration 196, loss = 1.26275231\n",
            "Iteration 197, loss = 1.25895122\n",
            "Iteration 198, loss = 1.25516154\n",
            "Iteration 199, loss = 1.25138523\n",
            "Iteration 200, loss = 1.24761412\n",
            "Iteration 201, loss = 1.24386835\n",
            "Iteration 202, loss = 1.24014584\n",
            "Iteration 203, loss = 1.23639907\n",
            "Iteration 204, loss = 1.23269607\n",
            "Iteration 205, loss = 1.22899511\n",
            "Iteration 206, loss = 1.22532849\n",
            "Iteration 207, loss = 1.22166485\n",
            "Iteration 208, loss = 1.21800874\n",
            "Iteration 209, loss = 1.21437988\n",
            "Iteration 210, loss = 1.21076408\n",
            "Iteration 211, loss = 1.20716767\n",
            "Iteration 212, loss = 1.20359257\n",
            "Iteration 213, loss = 1.20003063\n",
            "Iteration 214, loss = 1.19648389\n",
            "Iteration 215, loss = 1.19296208\n",
            "Iteration 216, loss = 1.18945273\n",
            "Iteration 217, loss = 1.18596288\n",
            "Iteration 218, loss = 1.18250063\n",
            "Iteration 219, loss = 1.17904709\n",
            "Iteration 220, loss = 1.17561961\n",
            "Iteration 221, loss = 1.17220172\n",
            "Iteration 222, loss = 1.16881351\n",
            "Iteration 223, loss = 1.16545515\n",
            "Iteration 224, loss = 1.16210343\n",
            "Iteration 225, loss = 1.15878195\n",
            "Iteration 226, loss = 1.15548221\n",
            "Iteration 227, loss = 1.15220412\n",
            "Iteration 228, loss = 1.14894476\n",
            "Iteration 229, loss = 1.14572186\n",
            "Iteration 230, loss = 1.14249916\n",
            "Iteration 231, loss = 1.13931050\n",
            "Iteration 232, loss = 1.13614359\n",
            "Iteration 233, loss = 1.13299996\n",
            "Iteration 234, loss = 1.12987850\n",
            "Iteration 235, loss = 1.12677950\n",
            "Iteration 236, loss = 1.12370728\n",
            "Iteration 237, loss = 1.12065366\n",
            "Iteration 238, loss = 1.11762234\n",
            "Iteration 239, loss = 1.11461870\n",
            "Iteration 240, loss = 1.11163459\n",
            "Iteration 241, loss = 1.10867483\n",
            "Iteration 242, loss = 1.10573847\n",
            "Iteration 243, loss = 1.10282654\n",
            "Iteration 244, loss = 1.09993801\n",
            "Iteration 245, loss = 1.09707325\n",
            "Iteration 246, loss = 1.09423163\n",
            "Iteration 247, loss = 1.09141635\n",
            "Iteration 248, loss = 1.08862322\n",
            "Iteration 249, loss = 1.08585549\n",
            "Iteration 250, loss = 1.08310873\n",
            "Iteration 251, loss = 1.08039125\n",
            "Iteration 252, loss = 1.07769365\n",
            "Iteration 253, loss = 1.07502055\n",
            "Iteration 254, loss = 1.07237290\n",
            "Iteration 255, loss = 1.06974866\n",
            "Iteration 256, loss = 1.06714897\n",
            "Iteration 257, loss = 1.06457264\n",
            "Iteration 258, loss = 1.06201880\n",
            "Iteration 259, loss = 1.05948700\n",
            "Iteration 260, loss = 1.05697669\n",
            "Iteration 261, loss = 1.05448762\n",
            "Iteration 262, loss = 1.05201840\n",
            "Iteration 263, loss = 1.04956952\n",
            "Iteration 264, loss = 1.04714049\n",
            "Iteration 265, loss = 1.04473119\n",
            "Iteration 266, loss = 1.04234774\n",
            "Iteration 267, loss = 1.03999191\n",
            "Iteration 268, loss = 1.03765645\n",
            "Iteration 269, loss = 1.03534136\n",
            "Iteration 270, loss = 1.03304661\n",
            "Iteration 271, loss = 1.03077331\n",
            "Iteration 272, loss = 1.02852048\n",
            "Iteration 273, loss = 1.02628866\n",
            "Iteration 274, loss = 1.02407733\n",
            "Iteration 275, loss = 1.02188546\n",
            "Iteration 276, loss = 1.01971299\n",
            "Iteration 277, loss = 1.01756013\n",
            "Iteration 278, loss = 1.01542558\n",
            "Iteration 279, loss = 1.01330922\n",
            "Iteration 280, loss = 1.01121073\n",
            "Iteration 281, loss = 1.00912985\n",
            "Iteration 282, loss = 1.00706661\n",
            "Iteration 283, loss = 1.00502033\n",
            "Iteration 284, loss = 1.00299101\n",
            "Iteration 285, loss = 1.00097847\n",
            "Iteration 286, loss = 0.99898254\n",
            "Iteration 287, loss = 0.99700312\n",
            "Iteration 288, loss = 0.99504022\n",
            "Iteration 289, loss = 0.99309323\n",
            "Iteration 290, loss = 0.99116207\n",
            "Iteration 291, loss = 0.98924653\n",
            "Iteration 292, loss = 0.98734663\n",
            "Iteration 293, loss = 0.98546191\n",
            "Iteration 294, loss = 0.98359214\n",
            "Iteration 295, loss = 0.98173711\n",
            "Iteration 296, loss = 0.97989689\n",
            "Iteration 297, loss = 0.97807103\n",
            "Iteration 298, loss = 0.97625932\n",
            "Iteration 299, loss = 0.97446159\n",
            "Iteration 300, loss = 0.97267942\n",
            "Iteration 301, loss = 0.97092345\n",
            "Iteration 302, loss = 0.96918219\n",
            "Iteration 303, loss = 0.96745502\n",
            "Iteration 304, loss = 0.96574273\n",
            "Iteration 305, loss = 0.96404576\n",
            "Iteration 306, loss = 0.96236429\n",
            "Iteration 307, loss = 0.96069801\n",
            "Iteration 308, loss = 0.95904579\n",
            "Iteration 309, loss = 0.95740745\n",
            "Iteration 310, loss = 0.95578374\n",
            "Iteration 311, loss = 0.95417230\n",
            "Iteration 312, loss = 0.95257542\n",
            "Iteration 313, loss = 0.95100098\n",
            "Iteration 314, loss = 0.94944007\n",
            "Iteration 315, loss = 0.94789310\n",
            "Iteration 316, loss = 0.94636016\n",
            "Iteration 317, loss = 0.94484100\n",
            "Iteration 318, loss = 0.94333514\n",
            "Iteration 319, loss = 0.94184198\n",
            "Iteration 320, loss = 0.94036086\n",
            "Iteration 321, loss = 0.93889119\n",
            "Iteration 322, loss = 0.93743249\n",
            "Iteration 323, loss = 0.93599250\n",
            "Iteration 324, loss = 0.93456362\n",
            "Iteration 325, loss = 0.93314598\n",
            "Iteration 326, loss = 0.93173999\n",
            "Iteration 327, loss = 0.93034534\n",
            "Iteration 328, loss = 0.92896201\n",
            "Iteration 329, loss = 0.92758968\n",
            "Iteration 330, loss = 0.92622800\n",
            "Iteration 331, loss = 0.92487656\n",
            "Iteration 332, loss = 0.92353500\n",
            "Iteration 333, loss = 0.92220319\n",
            "Iteration 334, loss = 0.92088065\n",
            "Iteration 335, loss = 0.91956719\n",
            "Iteration 336, loss = 0.91826286\n",
            "Iteration 337, loss = 0.91696738\n",
            "Iteration 338, loss = 0.91568077\n",
            "Iteration 339, loss = 0.91440283\n",
            "Iteration 340, loss = 0.91313355\n",
            "Iteration 341, loss = 0.91187300\n",
            "Iteration 342, loss = 0.91062106\n",
            "Iteration 343, loss = 0.90937725\n",
            "Iteration 344, loss = 0.90814243\n",
            "Iteration 345, loss = 0.90691453\n",
            "Iteration 346, loss = 0.90569493\n",
            "Iteration 347, loss = 0.90448607\n",
            "Iteration 348, loss = 0.90328557\n",
            "Iteration 349, loss = 0.90209254\n",
            "Iteration 350, loss = 0.90090580\n",
            "Iteration 351, loss = 0.89972831\n",
            "Iteration 352, loss = 0.89855837\n",
            "Iteration 353, loss = 0.89739792\n",
            "Iteration 354, loss = 0.89624512\n",
            "Iteration 355, loss = 0.89510161\n",
            "Iteration 356, loss = 0.89396277\n",
            "Iteration 357, loss = 0.89283200\n",
            "Iteration 358, loss = 0.89170795\n",
            "Iteration 359, loss = 0.89059047\n",
            "Iteration 360, loss = 0.88948040\n",
            "Iteration 361, loss = 0.88837538\n",
            "Iteration 362, loss = 0.88727721\n",
            "Iteration 363, loss = 0.88618588\n",
            "Iteration 364, loss = 0.88509999\n",
            "Iteration 365, loss = 0.88402048\n",
            "Iteration 366, loss = 0.88294695\n",
            "Iteration 367, loss = 0.88187955\n",
            "Iteration 368, loss = 0.88081824\n",
            "Iteration 369, loss = 0.87976357\n",
            "Iteration 370, loss = 0.87871383\n",
            "Iteration 371, loss = 0.87767345\n",
            "Iteration 372, loss = 0.87663391\n",
            "Iteration 373, loss = 0.87560448\n",
            "Iteration 374, loss = 0.87457769\n",
            "Iteration 375, loss = 0.87355900\n",
            "Iteration 376, loss = 0.87254490\n",
            "Iteration 377, loss = 0.87153606\n",
            "Iteration 378, loss = 0.87053337\n",
            "Iteration 379, loss = 0.86953670\n",
            "Iteration 380, loss = 0.86854396\n",
            "Iteration 381, loss = 0.86755623\n",
            "Iteration 382, loss = 0.86657591\n",
            "Iteration 383, loss = 0.86559860\n",
            "Iteration 384, loss = 0.86462676\n",
            "Iteration 385, loss = 0.86366054\n",
            "Iteration 386, loss = 0.86269805\n",
            "Iteration 387, loss = 0.86174067\n",
            "Iteration 388, loss = 0.86078859\n",
            "Iteration 389, loss = 0.85984059\n",
            "Iteration 390, loss = 0.85889703\n",
            "Iteration 391, loss = 0.85795787\n",
            "Iteration 392, loss = 0.85702344\n",
            "Iteration 393, loss = 0.85609319\n",
            "Iteration 394, loss = 0.85516718\n",
            "Iteration 395, loss = 0.85424533\n",
            "Iteration 396, loss = 0.85332821\n",
            "Iteration 397, loss = 0.85241461\n",
            "Iteration 398, loss = 0.85150531\n",
            "Iteration 399, loss = 0.85059999\n",
            "Iteration 400, loss = 0.84969882\n",
            "Iteration 401, loss = 0.84880218\n",
            "Iteration 402, loss = 0.84790970\n",
            "Iteration 403, loss = 0.84702327\n",
            "Iteration 404, loss = 0.84613824\n",
            "Iteration 405, loss = 0.84525690\n",
            "Iteration 406, loss = 0.84438190\n",
            "Iteration 407, loss = 0.84350882\n",
            "Iteration 408, loss = 0.84264044\n",
            "Iteration 409, loss = 0.84177581\n",
            "Iteration 410, loss = 0.84091490\n",
            "Iteration 411, loss = 0.84005786\n",
            "Iteration 412, loss = 0.83920361\n",
            "Iteration 413, loss = 0.83835328\n",
            "Iteration 414, loss = 0.83750680\n",
            "Iteration 415, loss = 0.83666327\n",
            "Iteration 416, loss = 0.83582339\n",
            "Iteration 417, loss = 0.83498689\n",
            "Iteration 418, loss = 0.83415372\n",
            "Iteration 419, loss = 0.83332383\n",
            "Iteration 420, loss = 0.83249720\n",
            "Iteration 421, loss = 0.83167410\n",
            "Iteration 422, loss = 0.83085389\n",
            "Iteration 423, loss = 0.83003697\n",
            "Iteration 424, loss = 0.82922317\n",
            "Iteration 425, loss = 0.82841243\n",
            "Iteration 426, loss = 0.82760475\n",
            "Iteration 427, loss = 0.82680035\n",
            "Iteration 428, loss = 0.82599879\n",
            "Iteration 429, loss = 0.82520023\n",
            "Iteration 430, loss = 0.82440459\n",
            "Iteration 431, loss = 0.82361183\n",
            "Iteration 432, loss = 0.82282238\n",
            "Iteration 433, loss = 0.82203532\n",
            "Iteration 434, loss = 0.82125125\n",
            "Iteration 435, loss = 0.82046994\n",
            "Iteration 436, loss = 0.81969180\n",
            "Iteration 437, loss = 0.81891600\n",
            "Iteration 438, loss = 0.81814304\n",
            "Iteration 439, loss = 0.81737271\n",
            "Iteration 440, loss = 0.81660562\n",
            "Iteration 441, loss = 0.81584046\n",
            "Iteration 442, loss = 0.81507816\n",
            "Iteration 443, loss = 0.81431887\n",
            "Iteration 444, loss = 0.81356175\n",
            "Iteration 445, loss = 0.81280724\n",
            "Iteration 446, loss = 0.81205540\n",
            "Iteration 447, loss = 0.81130619\n",
            "Iteration 448, loss = 0.81055923\n",
            "Iteration 449, loss = 0.80981475\n",
            "Iteration 450, loss = 0.80907309\n",
            "Iteration 451, loss = 0.80833346\n",
            "Iteration 452, loss = 0.80759643\n",
            "Iteration 453, loss = 0.80686178\n",
            "Iteration 454, loss = 0.80612927\n",
            "Iteration 455, loss = 0.80539988\n",
            "Iteration 456, loss = 0.80467181\n",
            "Iteration 457, loss = 0.80394691\n",
            "Iteration 458, loss = 0.80322381\n",
            "Iteration 459, loss = 0.80250279\n",
            "Iteration 460, loss = 0.80178471\n",
            "Iteration 461, loss = 0.80106813\n",
            "Iteration 462, loss = 0.80035435\n",
            "Iteration 463, loss = 0.79964231\n",
            "Iteration 464, loss = 0.79893278\n",
            "Iteration 465, loss = 0.79822507\n",
            "Iteration 466, loss = 0.79752002\n",
            "Iteration 467, loss = 0.79681645\n",
            "Iteration 468, loss = 0.79611903\n",
            "Iteration 469, loss = 0.79541847\n",
            "Iteration 470, loss = 0.79472278\n",
            "Iteration 471, loss = 0.79402840\n",
            "Iteration 472, loss = 0.79333745\n",
            "Iteration 473, loss = 0.79264812\n",
            "Iteration 474, loss = 0.79196025\n",
            "Iteration 475, loss = 0.79127642\n",
            "Iteration 476, loss = 0.79059396\n",
            "Iteration 477, loss = 0.78991366\n",
            "Iteration 478, loss = 0.78923592\n",
            "Iteration 479, loss = 0.78855963\n",
            "Iteration 480, loss = 0.78788563\n",
            "Iteration 481, loss = 0.78721331\n",
            "Iteration 482, loss = 0.78654322\n",
            "Iteration 483, loss = 0.78587533\n",
            "Iteration 484, loss = 0.78520950\n",
            "Iteration 485, loss = 0.78454523\n",
            "Iteration 486, loss = 0.78388284\n",
            "Iteration 487, loss = 0.78322222\n",
            "Iteration 488, loss = 0.78256407\n",
            "Iteration 489, loss = 0.78190682\n",
            "Iteration 490, loss = 0.78125197\n",
            "Iteration 491, loss = 0.78059885\n",
            "Iteration 492, loss = 0.77994743\n",
            "Iteration 493, loss = 0.77929774\n",
            "Iteration 494, loss = 0.77864978\n",
            "Iteration 495, loss = 0.77800355\n",
            "Iteration 496, loss = 0.77735947\n",
            "Iteration 497, loss = 0.77671650\n",
            "Iteration 498, loss = 0.77607549\n",
            "Iteration 499, loss = 0.77543718\n",
            "Iteration 500, loss = 0.77479919\n",
            "Iteration 501, loss = 0.77416367\n",
            "Iteration 502, loss = 0.77352963\n",
            "Iteration 503, loss = 0.77289714\n",
            "Iteration 504, loss = 0.77226619\n",
            "Iteration 505, loss = 0.77163677\n",
            "Iteration 506, loss = 0.77100974\n",
            "Iteration 507, loss = 0.77038308\n",
            "Iteration 508, loss = 0.76975845\n",
            "Iteration 509, loss = 0.76913560\n",
            "Iteration 510, loss = 0.76851412\n",
            "Iteration 511, loss = 0.76789410\n",
            "Iteration 512, loss = 0.76727552\n",
            "Iteration 513, loss = 0.76665837\n",
            "Iteration 514, loss = 0.76604265\n",
            "Iteration 515, loss = 0.76542840\n",
            "Iteration 516, loss = 0.76481738\n",
            "Iteration 517, loss = 0.76420505\n",
            "Iteration 518, loss = 0.76359593\n",
            "Iteration 519, loss = 0.76298829\n",
            "Iteration 520, loss = 0.76238201\n",
            "Iteration 521, loss = 0.76177717\n",
            "Iteration 522, loss = 0.76117372\n",
            "Iteration 523, loss = 0.76057165\n",
            "Iteration 524, loss = 0.75997142\n",
            "Iteration 525, loss = 0.75937284\n",
            "Iteration 526, loss = 0.75877560\n",
            "Iteration 527, loss = 0.75817970\n",
            "Iteration 528, loss = 0.75758511\n",
            "Iteration 529, loss = 0.75699184\n",
            "Iteration 530, loss = 0.75639986\n",
            "Iteration 531, loss = 0.75580917\n",
            "Iteration 532, loss = 0.75521975\n",
            "Iteration 533, loss = 0.75463158\n",
            "Iteration 534, loss = 0.75404467\n",
            "Iteration 535, loss = 0.75345898\n",
            "Iteration 536, loss = 0.75287462\n",
            "Iteration 537, loss = 0.75229169\n",
            "Iteration 538, loss = 0.75170989\n",
            "Iteration 539, loss = 0.75112928\n",
            "Iteration 540, loss = 0.75054982\n",
            "Iteration 541, loss = 0.74997150\n",
            "Iteration 542, loss = 0.74939432\n",
            "Iteration 543, loss = 0.74881826\n",
            "Iteration 544, loss = 0.74824333\n",
            "Iteration 545, loss = 0.74766950\n",
            "Iteration 546, loss = 0.74709678\n",
            "Iteration 547, loss = 0.74652516\n",
            "Iteration 548, loss = 0.74595474\n",
            "Iteration 549, loss = 0.74538558\n",
            "Iteration 550, loss = 0.74481735\n",
            "Iteration 551, loss = 0.74425079\n",
            "Iteration 552, loss = 0.74368449\n",
            "Iteration 553, loss = 0.74311956\n",
            "Iteration 554, loss = 0.74255590\n",
            "Iteration 555, loss = 0.74199321\n",
            "Iteration 556, loss = 0.74143145\n",
            "Iteration 557, loss = 0.74087061\n",
            "Iteration 558, loss = 0.74031117\n",
            "Iteration 559, loss = 0.73975236\n",
            "Iteration 560, loss = 0.73919455\n",
            "Iteration 561, loss = 0.73863830\n",
            "Iteration 562, loss = 0.73808227\n",
            "Iteration 563, loss = 0.73752755\n",
            "Iteration 564, loss = 0.73697409\n",
            "Iteration 565, loss = 0.73642120\n",
            "Iteration 566, loss = 0.73586909\n",
            "Iteration 567, loss = 0.73531870\n",
            "Iteration 568, loss = 0.73476827\n",
            "Iteration 569, loss = 0.73421953\n",
            "Iteration 570, loss = 0.73367126\n",
            "Iteration 571, loss = 0.73312371\n",
            "Iteration 572, loss = 0.73257787\n",
            "Iteration 573, loss = 0.73203203\n",
            "Iteration 574, loss = 0.73148755\n",
            "Iteration 575, loss = 0.73094361\n",
            "Iteration 576, loss = 0.73040088\n",
            "Iteration 577, loss = 0.72985863\n",
            "Iteration 578, loss = 0.72931766\n",
            "Iteration 579, loss = 0.72877706\n",
            "Iteration 580, loss = 0.72823773\n",
            "Iteration 581, loss = 0.72769880\n",
            "Iteration 582, loss = 0.72716106\n",
            "Iteration 583, loss = 0.72662375\n",
            "Iteration 584, loss = 0.72608761\n",
            "Iteration 585, loss = 0.72555180\n",
            "Iteration 586, loss = 0.72501735\n",
            "Iteration 587, loss = 0.72448302\n",
            "Iteration 588, loss = 0.72395022\n",
            "Iteration 589, loss = 0.72341734\n",
            "Iteration 590, loss = 0.72288581\n",
            "Iteration 591, loss = 0.72235460\n",
            "Iteration 592, loss = 0.72182422\n",
            "Iteration 593, loss = 0.72129473\n",
            "Iteration 594, loss = 0.72076559\n",
            "Iteration 595, loss = 0.72023798\n",
            "Iteration 596, loss = 0.71970999\n",
            "Iteration 597, loss = 0.71918360\n",
            "Iteration 598, loss = 0.71865740\n",
            "Iteration 599, loss = 0.71813173\n",
            "Iteration 600, loss = 0.71760695\n",
            "Iteration 601, loss = 0.71708281\n",
            "Iteration 602, loss = 0.71655912\n",
            "Iteration 603, loss = 0.71603680\n",
            "Iteration 604, loss = 0.71551411\n",
            "Iteration 605, loss = 0.71499264\n",
            "Iteration 606, loss = 0.71447183\n",
            "Iteration 607, loss = 0.71395141\n",
            "Iteration 608, loss = 0.71343151\n",
            "Iteration 609, loss = 0.71291238\n",
            "Iteration 610, loss = 0.71239388\n",
            "Iteration 611, loss = 0.71187580\n",
            "Iteration 612, loss = 0.71135822\n",
            "Iteration 613, loss = 0.71084191\n",
            "Iteration 614, loss = 0.71032517\n",
            "Iteration 615, loss = 0.70980937\n",
            "Iteration 616, loss = 0.70929438\n",
            "Iteration 617, loss = 0.70877974\n",
            "Iteration 618, loss = 0.70826561\n",
            "Iteration 619, loss = 0.70775194\n",
            "Iteration 620, loss = 0.70723875\n",
            "Iteration 621, loss = 0.70672627\n",
            "Iteration 622, loss = 0.70621425\n",
            "Iteration 623, loss = 0.70570266\n",
            "Iteration 624, loss = 0.70519152\n",
            "Iteration 625, loss = 0.70468084\n",
            "Iteration 626, loss = 0.70417070\n",
            "Iteration 627, loss = 0.70366121\n",
            "Iteration 628, loss = 0.70315202\n",
            "Iteration 629, loss = 0.70264326\n",
            "Iteration 630, loss = 0.70213495\n",
            "Iteration 631, loss = 0.70162712\n",
            "Iteration 632, loss = 0.70111973\n",
            "Iteration 633, loss = 0.70061285\n",
            "Iteration 634, loss = 0.70010647\n",
            "Iteration 635, loss = 0.69960043\n",
            "Iteration 636, loss = 0.69909519\n",
            "Iteration 637, loss = 0.69858999\n",
            "Iteration 638, loss = 0.69808541\n",
            "Iteration 639, loss = 0.69758116\n",
            "Iteration 640, loss = 0.69707813\n",
            "Iteration 641, loss = 0.69657468\n",
            "Iteration 642, loss = 0.69607245\n",
            "Iteration 643, loss = 0.69557044\n",
            "Iteration 644, loss = 0.69506826\n",
            "Iteration 645, loss = 0.69456783\n",
            "Iteration 646, loss = 0.69406633\n",
            "Iteration 647, loss = 0.69356641\n",
            "Iteration 648, loss = 0.69306570\n",
            "Iteration 649, loss = 0.69256651\n",
            "Iteration 650, loss = 0.69206710\n",
            "Iteration 651, loss = 0.69156806\n",
            "Iteration 652, loss = 0.69106934\n",
            "Iteration 653, loss = 0.69057134\n",
            "Iteration 654, loss = 0.69007354\n",
            "Iteration 655, loss = 0.68957583\n",
            "Iteration 656, loss = 0.68907833\n",
            "Iteration 657, loss = 0.68858113\n",
            "Iteration 658, loss = 0.68808541\n",
            "Iteration 659, loss = 0.68758825\n",
            "Iteration 660, loss = 0.68709209\n",
            "Iteration 661, loss = 0.68659615\n",
            "Iteration 662, loss = 0.68610045\n",
            "Iteration 663, loss = 0.68560516\n",
            "Iteration 664, loss = 0.68511038\n",
            "Iteration 665, loss = 0.68461560\n",
            "Iteration 666, loss = 0.68412096\n",
            "Iteration 667, loss = 0.68362651\n",
            "Iteration 668, loss = 0.68313227\n",
            "Iteration 669, loss = 0.68263826\n",
            "Iteration 670, loss = 0.68214471\n",
            "Iteration 671, loss = 0.68165139\n",
            "Iteration 672, loss = 0.68115848\n",
            "Iteration 673, loss = 0.68066523\n",
            "Iteration 674, loss = 0.68017243\n",
            "Iteration 675, loss = 0.67967983\n",
            "Iteration 676, loss = 0.67918775\n",
            "Iteration 677, loss = 0.67869553\n",
            "Iteration 678, loss = 0.67820404\n",
            "Iteration 679, loss = 0.67771196\n",
            "Iteration 680, loss = 0.67722035\n",
            "Iteration 681, loss = 0.67672889\n",
            "Iteration 682, loss = 0.67623757\n",
            "Iteration 683, loss = 0.67574653\n",
            "Iteration 684, loss = 0.67525617\n",
            "Iteration 685, loss = 0.67476511\n",
            "Iteration 686, loss = 0.67427449\n",
            "Iteration 687, loss = 0.67378395\n",
            "Iteration 688, loss = 0.67329347\n",
            "Iteration 689, loss = 0.67280366\n",
            "Iteration 690, loss = 0.67231331\n",
            "Iteration 691, loss = 0.67182432\n",
            "Iteration 692, loss = 0.67133445\n",
            "Iteration 693, loss = 0.67084556\n",
            "Iteration 694, loss = 0.67035662\n",
            "Iteration 695, loss = 0.66986768\n",
            "Iteration 696, loss = 0.66937874\n",
            "Iteration 697, loss = 0.66889013\n",
            "Iteration 698, loss = 0.66840149\n",
            "Iteration 699, loss = 0.66791286\n",
            "Iteration 700, loss = 0.66742421\n",
            "Iteration 701, loss = 0.66693601\n",
            "Iteration 702, loss = 0.66644760\n",
            "Iteration 703, loss = 0.66595920\n",
            "Iteration 704, loss = 0.66547105\n",
            "Iteration 705, loss = 0.66498336\n",
            "Iteration 706, loss = 0.66449487\n",
            "Iteration 707, loss = 0.66400700\n",
            "Iteration 708, loss = 0.66351893\n",
            "Iteration 709, loss = 0.66303074\n",
            "Iteration 710, loss = 0.66254349\n",
            "Iteration 711, loss = 0.66205480\n",
            "Iteration 712, loss = 0.66156699\n",
            "Iteration 713, loss = 0.66107921\n",
            "Iteration 714, loss = 0.66059117\n",
            "Iteration 715, loss = 0.66010294\n",
            "Iteration 716, loss = 0.65961572\n",
            "Iteration 717, loss = 0.65912724\n",
            "Iteration 718, loss = 0.65864027\n",
            "Iteration 719, loss = 0.65815229\n",
            "Iteration 720, loss = 0.65766403\n",
            "Iteration 721, loss = 0.65717577\n",
            "Iteration 722, loss = 0.65668780\n",
            "Iteration 723, loss = 0.65620127\n",
            "Iteration 724, loss = 0.65571454\n",
            "Iteration 725, loss = 0.65522693\n",
            "Iteration 726, loss = 0.65473871\n",
            "Iteration 727, loss = 0.65425000\n",
            "Iteration 728, loss = 0.65376090\n",
            "Iteration 729, loss = 0.65327182\n",
            "Iteration 730, loss = 0.65278390\n",
            "Iteration 731, loss = 0.65229681\n",
            "Iteration 732, loss = 0.65180825\n",
            "Iteration 733, loss = 0.65131933\n",
            "Iteration 734, loss = 0.65083082\n",
            "Iteration 735, loss = 0.65034317\n",
            "Iteration 736, loss = 0.64985433\n",
            "Iteration 737, loss = 0.64936508\n",
            "Iteration 738, loss = 0.64887623\n",
            "Iteration 739, loss = 0.64838819\n",
            "Iteration 740, loss = 0.64789899\n",
            "Iteration 741, loss = 0.64740941\n",
            "Iteration 742, loss = 0.64691974\n",
            "Iteration 743, loss = 0.64643044\n",
            "Iteration 744, loss = 0.64594127\n",
            "Iteration 745, loss = 0.64545125\n",
            "Iteration 746, loss = 0.64496195\n",
            "Iteration 747, loss = 0.64447173\n",
            "Iteration 748, loss = 0.64398179\n",
            "Iteration 749, loss = 0.64349181\n",
            "Iteration 750, loss = 0.64300123\n",
            "Iteration 751, loss = 0.64251104\n",
            "Iteration 752, loss = 0.64202008\n",
            "Iteration 753, loss = 0.64152936\n",
            "Iteration 754, loss = 0.64103853\n",
            "Iteration 755, loss = 0.64054712\n",
            "Iteration 756, loss = 0.64005595\n",
            "Iteration 757, loss = 0.63956434\n",
            "Iteration 758, loss = 0.63907235\n",
            "Iteration 759, loss = 0.63858109\n",
            "Iteration 760, loss = 0.63808839\n",
            "Iteration 761, loss = 0.63759619\n",
            "Iteration 762, loss = 0.63710394\n",
            "Iteration 763, loss = 0.63661109\n",
            "Iteration 764, loss = 0.63611791\n",
            "Iteration 765, loss = 0.63562512\n",
            "Iteration 766, loss = 0.63513143\n",
            "Iteration 767, loss = 0.63463764\n",
            "Iteration 768, loss = 0.63414434\n",
            "Iteration 769, loss = 0.63364995\n",
            "Iteration 770, loss = 0.63315553\n",
            "Iteration 771, loss = 0.63266096\n",
            "Iteration 772, loss = 0.63216651\n",
            "Iteration 773, loss = 0.63167162\n",
            "Iteration 774, loss = 0.63117637\n",
            "Iteration 775, loss = 0.63068081\n",
            "Iteration 776, loss = 0.63018598\n",
            "Iteration 777, loss = 0.62968949\n",
            "Iteration 778, loss = 0.62919331\n",
            "Iteration 779, loss = 0.62869725\n",
            "Iteration 780, loss = 0.62820065\n",
            "Iteration 781, loss = 0.62770376\n",
            "Iteration 782, loss = 0.62720651\n",
            "Iteration 783, loss = 0.62670893\n",
            "Iteration 784, loss = 0.62621112\n",
            "Iteration 785, loss = 0.62571340\n",
            "Iteration 786, loss = 0.62521509\n",
            "Iteration 787, loss = 0.62471643\n",
            "Iteration 788, loss = 0.62421744\n",
            "Iteration 789, loss = 0.62371813\n",
            "Iteration 790, loss = 0.62321898\n",
            "Iteration 791, loss = 0.62271902\n",
            "Iteration 792, loss = 0.62221893\n",
            "Iteration 793, loss = 0.62171848\n",
            "Iteration 794, loss = 0.62121770\n",
            "Iteration 795, loss = 0.62071658\n",
            "Iteration 796, loss = 0.62021513\n",
            "Iteration 797, loss = 0.61971366\n",
            "Iteration 798, loss = 0.61921156\n",
            "Iteration 799, loss = 0.61870923\n",
            "Iteration 800, loss = 0.61820654\n",
            "Iteration 801, loss = 0.61770348\n",
            "Iteration 802, loss = 0.61720007\n",
            "Iteration 803, loss = 0.61669630\n",
            "Iteration 804, loss = 0.61619219\n",
            "Iteration 805, loss = 0.61568773\n",
            "Iteration 806, loss = 0.61518292\n",
            "Iteration 807, loss = 0.61467777\n",
            "Iteration 808, loss = 0.61417226\n",
            "Iteration 809, loss = 0.61366640\n",
            "Iteration 810, loss = 0.61316052\n",
            "Iteration 811, loss = 0.61265363\n",
            "Iteration 812, loss = 0.61214669\n",
            "Iteration 813, loss = 0.61163936\n",
            "Iteration 814, loss = 0.61113164\n",
            "Iteration 815, loss = 0.61062352\n",
            "Iteration 816, loss = 0.61011500\n",
            "Iteration 817, loss = 0.60960609\n",
            "Iteration 818, loss = 0.60909686\n",
            "Iteration 819, loss = 0.60858730\n",
            "Iteration 820, loss = 0.60807735\n",
            "Iteration 821, loss = 0.60756700\n",
            "Iteration 822, loss = 0.60705625\n",
            "Iteration 823, loss = 0.60654511\n",
            "Iteration 824, loss = 0.60603356\n",
            "Iteration 825, loss = 0.60552159\n",
            "Iteration 826, loss = 0.60500922\n",
            "Iteration 827, loss = 0.60449642\n",
            "Iteration 828, loss = 0.60398321\n",
            "Iteration 829, loss = 0.60346957\n",
            "Iteration 830, loss = 0.60295550\n",
            "Iteration 831, loss = 0.60244100\n",
            "Iteration 832, loss = 0.60192606\n",
            "Iteration 833, loss = 0.60141068\n",
            "Iteration 834, loss = 0.60089486\n",
            "Iteration 835, loss = 0.60037859\n",
            "Iteration 836, loss = 0.59986187\n",
            "Iteration 837, loss = 0.59934469\n",
            "Iteration 838, loss = 0.59882706\n",
            "Iteration 839, loss = 0.59830897\n",
            "Iteration 840, loss = 0.59779041\n",
            "Iteration 841, loss = 0.59727138\n",
            "Iteration 842, loss = 0.59675188\n",
            "Iteration 843, loss = 0.59623190\n",
            "Iteration 844, loss = 0.59571145\n",
            "Iteration 845, loss = 0.59519051\n",
            "Iteration 846, loss = 0.59466909\n",
            "Iteration 847, loss = 0.59414718\n",
            "Iteration 848, loss = 0.59362478\n",
            "Iteration 849, loss = 0.59310188\n",
            "Iteration 850, loss = 0.59257849\n",
            "Iteration 851, loss = 0.59205459\n",
            "Iteration 852, loss = 0.59153019\n",
            "Iteration 853, loss = 0.59100529\n",
            "Iteration 854, loss = 0.59047987\n",
            "Iteration 855, loss = 0.58995394\n",
            "Iteration 856, loss = 0.58942750\n",
            "Iteration 857, loss = 0.58890054\n",
            "Iteration 858, loss = 0.58837305\n",
            "Iteration 859, loss = 0.58784504\n",
            "Iteration 860, loss = 0.58731651\n",
            "Iteration 861, loss = 0.58678745\n",
            "Iteration 862, loss = 0.58625785\n",
            "Iteration 863, loss = 0.58572773\n",
            "Iteration 864, loss = 0.58519706\n",
            "Iteration 865, loss = 0.58466586\n",
            "Iteration 866, loss = 0.58413411\n",
            "Iteration 867, loss = 0.58360182\n",
            "Iteration 868, loss = 0.58306899\n",
            "Iteration 869, loss = 0.58253560\n",
            "Iteration 870, loss = 0.58200167\n",
            "Iteration 871, loss = 0.58146718\n",
            "Iteration 872, loss = 0.58093214\n",
            "Iteration 873, loss = 0.58039654\n",
            "Iteration 874, loss = 0.57986038\n",
            "Iteration 875, loss = 0.57932366\n",
            "Iteration 876, loss = 0.57878638\n",
            "Iteration 877, loss = 0.57824853\n",
            "Iteration 878, loss = 0.57771011\n",
            "Iteration 879, loss = 0.57717113\n",
            "Iteration 880, loss = 0.57663157\n",
            "Iteration 881, loss = 0.57609148\n",
            "Iteration 882, loss = 0.57555088\n",
            "Iteration 883, loss = 0.57500971\n",
            "Iteration 884, loss = 0.57446796\n",
            "Iteration 885, loss = 0.57392771\n",
            "Iteration 886, loss = 0.57338590\n",
            "Iteration 887, loss = 0.57284278\n",
            "Iteration 888, loss = 0.57229975\n",
            "Iteration 889, loss = 0.57175640\n",
            "Iteration 890, loss = 0.57121305\n",
            "Iteration 891, loss = 0.57066869\n",
            "Iteration 892, loss = 0.57012386\n",
            "Iteration 893, loss = 0.56957891\n",
            "Iteration 894, loss = 0.56903255\n",
            "Iteration 895, loss = 0.56848718\n",
            "Iteration 896, loss = 0.56793974\n",
            "Iteration 897, loss = 0.56739272\n",
            "Iteration 898, loss = 0.56684455\n",
            "Iteration 899, loss = 0.56629661\n",
            "Iteration 900, loss = 0.56574745\n",
            "Iteration 901, loss = 0.56519860\n",
            "Iteration 902, loss = 0.56464862\n",
            "Iteration 903, loss = 0.56409840\n",
            "Iteration 904, loss = 0.56354794\n",
            "Iteration 905, loss = 0.56299618\n",
            "Iteration 906, loss = 0.56244543\n",
            "Iteration 907, loss = 0.56189261\n",
            "Iteration 908, loss = 0.56134003\n",
            "Iteration 909, loss = 0.56078624\n",
            "Iteration 910, loss = 0.56023345\n",
            "Iteration 911, loss = 0.55967875\n",
            "Iteration 912, loss = 0.55912391\n",
            "Iteration 913, loss = 0.55856866\n",
            "Iteration 914, loss = 0.55801293\n",
            "Iteration 915, loss = 0.55745688\n",
            "Iteration 916, loss = 0.55689976\n",
            "Iteration 917, loss = 0.55634330\n",
            "Iteration 918, loss = 0.55578483\n",
            "Iteration 919, loss = 0.55522700\n",
            "Iteration 920, loss = 0.55466829\n",
            "Iteration 921, loss = 0.55410885\n",
            "Iteration 922, loss = 0.55354916\n",
            "Iteration 923, loss = 0.55298921\n",
            "Iteration 924, loss = 0.55242827\n",
            "Iteration 925, loss = 0.55186695\n",
            "Iteration 926, loss = 0.55130564\n",
            "Iteration 927, loss = 0.55074325\n",
            "Iteration 928, loss = 0.55018024\n",
            "Iteration 929, loss = 0.54961801\n",
            "Iteration 930, loss = 0.54905351\n",
            "Iteration 931, loss = 0.54848922\n",
            "Iteration 932, loss = 0.54792527\n",
            "Iteration 933, loss = 0.54736012\n",
            "Iteration 934, loss = 0.54679433\n",
            "Iteration 935, loss = 0.54622797\n",
            "Iteration 936, loss = 0.54566109\n",
            "Iteration 937, loss = 0.54509506\n",
            "Iteration 938, loss = 0.54452656\n",
            "Iteration 939, loss = 0.54395844\n",
            "Iteration 940, loss = 0.54338990\n",
            "Iteration 941, loss = 0.54282131\n",
            "Iteration 942, loss = 0.54225185\n",
            "Iteration 943, loss = 0.54168183\n",
            "Iteration 944, loss = 0.54111127\n",
            "Iteration 945, loss = 0.54054022\n",
            "Iteration 946, loss = 0.53996868\n",
            "Iteration 947, loss = 0.53939669\n",
            "Iteration 948, loss = 0.53882503\n",
            "Iteration 949, loss = 0.53825169\n",
            "Iteration 950, loss = 0.53767843\n",
            "Iteration 951, loss = 0.53710467\n",
            "Iteration 952, loss = 0.53653043\n",
            "Iteration 953, loss = 0.53595571\n",
            "Iteration 954, loss = 0.53538053\n",
            "Iteration 955, loss = 0.53480489\n",
            "Iteration 956, loss = 0.53422880\n",
            "Iteration 957, loss = 0.53365227\n",
            "Iteration 958, loss = 0.53307530\n",
            "Iteration 959, loss = 0.53249791\n",
            "Iteration 960, loss = 0.53192006\n",
            "Iteration 961, loss = 0.53134176\n",
            "Iteration 962, loss = 0.53076298\n",
            "Iteration 963, loss = 0.53018372\n",
            "Iteration 964, loss = 0.52960472\n",
            "Iteration 965, loss = 0.52902407\n",
            "Iteration 966, loss = 0.52844368\n",
            "Iteration 967, loss = 0.52786285\n",
            "Iteration 968, loss = 0.52728196\n",
            "Iteration 969, loss = 0.52670315\n",
            "Iteration 970, loss = 0.52611964\n",
            "Iteration 971, loss = 0.52553904\n",
            "Iteration 972, loss = 0.52495774\n",
            "Iteration 973, loss = 0.52437623\n",
            "Iteration 974, loss = 0.52379434\n",
            "Iteration 975, loss = 0.52321209\n",
            "Iteration 976, loss = 0.52262948\n",
            "Iteration 977, loss = 0.52204653\n",
            "Iteration 978, loss = 0.52146323\n",
            "Iteration 979, loss = 0.52087957\n",
            "Iteration 980, loss = 0.52029554\n",
            "Iteration 981, loss = 0.51971113\n",
            "Iteration 982, loss = 0.51912669\n",
            "Iteration 983, loss = 0.51854153\n",
            "Iteration 984, loss = 0.51795617\n",
            "Iteration 985, loss = 0.51737041\n",
            "Iteration 986, loss = 0.51678422\n",
            "Iteration 987, loss = 0.51619760\n",
            "Iteration 988, loss = 0.51561056\n",
            "Iteration 989, loss = 0.51502397\n",
            "Iteration 990, loss = 0.51443601\n",
            "Iteration 991, loss = 0.51384804\n",
            "Iteration 992, loss = 0.51325960\n",
            "Iteration 993, loss = 0.51267174\n",
            "Iteration 994, loss = 0.51208234\n",
            "Iteration 995, loss = 0.51149295\n",
            "Iteration 996, loss = 0.51090330\n",
            "Iteration 997, loss = 0.51031380\n",
            "Iteration 998, loss = 0.50972338\n",
            "Iteration 999, loss = 0.50913244\n",
            "Iteration 1000, loss = 0.50854186\n",
            "Iteration 1001, loss = 0.50795032\n",
            "Iteration 1002, loss = 0.50735845\n",
            "Iteration 1003, loss = 0.50676730\n",
            "Iteration 1004, loss = 0.50617446\n",
            "Iteration 1005, loss = 0.50558178\n",
            "Iteration 1006, loss = 0.50498961\n",
            "Iteration 1007, loss = 0.50439623\n",
            "Iteration 1008, loss = 0.50380228\n",
            "Iteration 1009, loss = 0.50320933\n",
            "Iteration 1010, loss = 0.50261440\n",
            "Iteration 1011, loss = 0.50202084\n",
            "Iteration 1012, loss = 0.50142593\n",
            "Iteration 1013, loss = 0.50083038\n",
            "Iteration 1014, loss = 0.50023619\n",
            "Iteration 1015, loss = 0.49964004\n",
            "Iteration 1016, loss = 0.49904431\n",
            "Iteration 1017, loss = 0.49844779\n",
            "Iteration 1018, loss = 0.49785223\n",
            "Iteration 1019, loss = 0.49725500\n",
            "Iteration 1020, loss = 0.49665874\n",
            "Iteration 1021, loss = 0.49606106\n",
            "Iteration 1022, loss = 0.49546415\n",
            "Iteration 1023, loss = 0.49486607\n",
            "Iteration 1024, loss = 0.49426859\n",
            "Iteration 1025, loss = 0.49366996\n",
            "Iteration 1026, loss = 0.49307214\n",
            "Iteration 1027, loss = 0.49247319\n",
            "Iteration 1028, loss = 0.49187437\n",
            "Iteration 1029, loss = 0.49127572\n",
            "Iteration 1030, loss = 0.49067612\n",
            "Iteration 1031, loss = 0.49007677\n",
            "Iteration 1032, loss = 0.48947705\n",
            "Iteration 1033, loss = 0.48887680\n",
            "Iteration 1034, loss = 0.48827757\n",
            "Iteration 1035, loss = 0.48767650\n",
            "Iteration 1036, loss = 0.48707615\n",
            "Iteration 1037, loss = 0.48647579\n",
            "Iteration 1038, loss = 0.48587478\n",
            "Iteration 1039, loss = 0.48527343\n",
            "Iteration 1040, loss = 0.48467240\n",
            "Iteration 1041, loss = 0.48407095\n",
            "Iteration 1042, loss = 0.48346921\n",
            "Iteration 1043, loss = 0.48286724\n",
            "Iteration 1044, loss = 0.48226616\n",
            "Iteration 1045, loss = 0.48166346\n",
            "Iteration 1046, loss = 0.48106118\n",
            "Iteration 1047, loss = 0.48045868\n",
            "Iteration 1048, loss = 0.47985650\n",
            "Iteration 1049, loss = 0.47925382\n",
            "Iteration 1050, loss = 0.47865102\n",
            "Iteration 1051, loss = 0.47804802\n",
            "Iteration 1052, loss = 0.47744487\n",
            "Iteration 1053, loss = 0.47684159\n",
            "Iteration 1054, loss = 0.47623821\n",
            "Iteration 1055, loss = 0.47563555\n",
            "Iteration 1056, loss = 0.47503146\n",
            "Iteration 1057, loss = 0.47442787\n",
            "Iteration 1058, loss = 0.47382415\n",
            "Iteration 1059, loss = 0.47322040\n",
            "Iteration 1060, loss = 0.47261657\n",
            "Iteration 1061, loss = 0.47201266\n",
            "Iteration 1062, loss = 0.47140868\n",
            "Iteration 1063, loss = 0.47080472\n",
            "Iteration 1064, loss = 0.47020064\n",
            "Iteration 1065, loss = 0.46959648\n",
            "Iteration 1066, loss = 0.46899224\n",
            "Iteration 1067, loss = 0.46838799\n",
            "Iteration 1068, loss = 0.46778377\n",
            "Iteration 1069, loss = 0.46718251\n",
            "Iteration 1070, loss = 0.46657829\n",
            "Iteration 1071, loss = 0.46597678\n",
            "Iteration 1072, loss = 0.46537541\n",
            "Iteration 1073, loss = 0.46477416\n",
            "Iteration 1074, loss = 0.46417304\n",
            "Iteration 1075, loss = 0.46357209\n",
            "Iteration 1076, loss = 0.46297123\n",
            "Iteration 1077, loss = 0.46237053\n",
            "Iteration 1078, loss = 0.46176995\n",
            "Iteration 1079, loss = 0.46116950\n",
            "Iteration 1080, loss = 0.46056916\n",
            "Iteration 1081, loss = 0.45996894\n",
            "Iteration 1082, loss = 0.45936883\n",
            "Iteration 1083, loss = 0.45876883\n",
            "Iteration 1084, loss = 0.45816913\n",
            "Iteration 1085, loss = 0.45756930\n",
            "Iteration 1086, loss = 0.45696969\n",
            "Iteration 1087, loss = 0.45637016\n",
            "Iteration 1088, loss = 0.45577070\n",
            "Iteration 1089, loss = 0.45517157\n",
            "Iteration 1090, loss = 0.45457227\n",
            "Iteration 1091, loss = 0.45397313\n",
            "Iteration 1092, loss = 0.45337409\n",
            "Iteration 1093, loss = 0.45277532\n",
            "Iteration 1094, loss = 0.45217644\n",
            "Iteration 1095, loss = 0.45157763\n",
            "Iteration 1096, loss = 0.45097912\n",
            "Iteration 1097, loss = 0.45038046\n",
            "Iteration 1098, loss = 0.44978214\n",
            "Iteration 1099, loss = 0.44918366\n",
            "Iteration 1100, loss = 0.44858547\n",
            "Iteration 1101, loss = 0.44798726\n",
            "Iteration 1102, loss = 0.44738913\n",
            "Iteration 1103, loss = 0.44679126\n",
            "Iteration 1104, loss = 0.44619323\n",
            "Iteration 1105, loss = 0.44559562\n",
            "Iteration 1106, loss = 0.44499774\n",
            "Iteration 1107, loss = 0.44440038\n",
            "Iteration 1108, loss = 0.44380403\n",
            "Iteration 1109, loss = 0.44320624\n",
            "Iteration 1110, loss = 0.44260983\n",
            "Iteration 1111, loss = 0.44201292\n",
            "Iteration 1112, loss = 0.44141653\n",
            "Iteration 1113, loss = 0.44082044\n",
            "Iteration 1114, loss = 0.44022437\n",
            "Iteration 1115, loss = 0.43962805\n",
            "Iteration 1116, loss = 0.43903257\n",
            "Iteration 1117, loss = 0.43843709\n",
            "Iteration 1118, loss = 0.43784124\n",
            "Iteration 1119, loss = 0.43724666\n",
            "Iteration 1120, loss = 0.43665137\n",
            "Iteration 1121, loss = 0.43605674\n",
            "Iteration 1122, loss = 0.43546289\n",
            "Iteration 1123, loss = 0.43486810\n",
            "Iteration 1124, loss = 0.43427460\n",
            "Iteration 1125, loss = 0.43368041\n",
            "Iteration 1126, loss = 0.43308756\n",
            "Iteration 1127, loss = 0.43249381\n",
            "Iteration 1128, loss = 0.43190090\n",
            "Iteration 1129, loss = 0.43130805\n",
            "Iteration 1130, loss = 0.43071569\n",
            "Iteration 1131, loss = 0.43012313\n",
            "Iteration 1132, loss = 0.42953169\n",
            "Iteration 1133, loss = 0.42894001\n",
            "Iteration 1134, loss = 0.42834886\n",
            "Iteration 1135, loss = 0.42775741\n",
            "Iteration 1136, loss = 0.42716701\n",
            "Iteration 1137, loss = 0.42657629\n",
            "Iteration 1138, loss = 0.42598602\n",
            "Iteration 1139, loss = 0.42539603\n",
            "Iteration 1140, loss = 0.42480735\n",
            "Iteration 1141, loss = 0.42421757\n",
            "Iteration 1142, loss = 0.42362852\n",
            "Iteration 1143, loss = 0.42304043\n",
            "Iteration 1144, loss = 0.42245195\n",
            "Iteration 1145, loss = 0.42186398\n",
            "Iteration 1146, loss = 0.42127650\n",
            "Iteration 1147, loss = 0.42068951\n",
            "Iteration 1148, loss = 0.42010287\n",
            "Iteration 1149, loss = 0.41951619\n",
            "Iteration 1150, loss = 0.41893003\n",
            "Iteration 1151, loss = 0.41834424\n",
            "Iteration 1152, loss = 0.41775882\n",
            "Iteration 1153, loss = 0.41717394\n",
            "Iteration 1154, loss = 0.41658956\n",
            "Iteration 1155, loss = 0.41600509\n",
            "Iteration 1156, loss = 0.41542126\n",
            "Iteration 1157, loss = 0.41483785\n",
            "Iteration 1158, loss = 0.41425486\n",
            "Iteration 1159, loss = 0.41367225\n",
            "Iteration 1160, loss = 0.41309002\n",
            "Iteration 1161, loss = 0.41250818\n",
            "Iteration 1162, loss = 0.41192675\n",
            "Iteration 1163, loss = 0.41134573\n",
            "Iteration 1164, loss = 0.41076520\n",
            "Iteration 1165, loss = 0.41018507\n",
            "Iteration 1166, loss = 0.40960535\n",
            "Iteration 1167, loss = 0.40902604\n",
            "Iteration 1168, loss = 0.40844713\n",
            "Iteration 1169, loss = 0.40786879\n",
            "Iteration 1170, loss = 0.40729074\n",
            "Iteration 1171, loss = 0.40671314\n",
            "Iteration 1172, loss = 0.40613595\n",
            "Iteration 1173, loss = 0.40555918\n",
            "Iteration 1174, loss = 0.40498290\n",
            "Iteration 1175, loss = 0.40440706\n",
            "Iteration 1176, loss = 0.40383158\n",
            "Iteration 1177, loss = 0.40325652\n",
            "Iteration 1178, loss = 0.40268189\n",
            "Iteration 1179, loss = 0.40210774\n",
            "Iteration 1180, loss = 0.40153403\n",
            "Iteration 1181, loss = 0.40096071\n",
            "Iteration 1182, loss = 0.40038781\n",
            "Iteration 1183, loss = 0.39981535\n",
            "Iteration 1184, loss = 0.39924335\n",
            "Iteration 1185, loss = 0.39867182\n",
            "Iteration 1186, loss = 0.39810069\n",
            "Iteration 1187, loss = 0.39752999\n",
            "Iteration 1188, loss = 0.39695972\n",
            "Iteration 1189, loss = 0.39638991\n",
            "Iteration 1190, loss = 0.39582063\n",
            "Iteration 1191, loss = 0.39525175\n",
            "Iteration 1192, loss = 0.39468327\n",
            "Iteration 1193, loss = 0.39411526\n",
            "Iteration 1194, loss = 0.39354770\n",
            "Iteration 1195, loss = 0.39298070\n",
            "Iteration 1196, loss = 0.39241406\n",
            "Iteration 1197, loss = 0.39184792\n",
            "Iteration 1198, loss = 0.39128223\n",
            "Iteration 1199, loss = 0.39071701\n",
            "Iteration 1200, loss = 0.39015225\n",
            "Iteration 1201, loss = 0.38958797\n",
            "Iteration 1202, loss = 0.38902428\n",
            "Iteration 1203, loss = 0.38846093\n",
            "Iteration 1204, loss = 0.38789810\n",
            "Iteration 1205, loss = 0.38733575\n",
            "Iteration 1206, loss = 0.38677389\n",
            "Iteration 1207, loss = 0.38621251\n",
            "Iteration 1208, loss = 0.38565162\n",
            "Iteration 1209, loss = 0.38509124\n",
            "Iteration 1210, loss = 0.38453135\n",
            "Iteration 1211, loss = 0.38397207\n",
            "Iteration 1212, loss = 0.38341315\n",
            "Iteration 1213, loss = 0.38285477\n",
            "Iteration 1214, loss = 0.38229692\n",
            "Iteration 1215, loss = 0.38173957\n",
            "Iteration 1216, loss = 0.38118275\n",
            "Iteration 1217, loss = 0.38062643\n",
            "Iteration 1218, loss = 0.38007064\n",
            "Iteration 1219, loss = 0.37951537\n",
            "Iteration 1220, loss = 0.37896063\n",
            "Iteration 1221, loss = 0.37840643\n",
            "Iteration 1222, loss = 0.37785276\n",
            "Iteration 1223, loss = 0.37729963\n",
            "Iteration 1224, loss = 0.37674704\n",
            "Iteration 1225, loss = 0.37619501\n",
            "Iteration 1226, loss = 0.37564351\n",
            "Iteration 1227, loss = 0.37509256\n",
            "Iteration 1228, loss = 0.37454218\n",
            "Iteration 1229, loss = 0.37399234\n",
            "Iteration 1230, loss = 0.37344307\n",
            "Iteration 1231, loss = 0.37289436\n",
            "Iteration 1232, loss = 0.37234622\n",
            "Iteration 1233, loss = 0.37179865\n",
            "Iteration 1234, loss = 0.37125165\n",
            "Iteration 1235, loss = 0.37070522\n",
            "Iteration 1236, loss = 0.37015939\n",
            "Iteration 1237, loss = 0.36961413\n",
            "Iteration 1238, loss = 0.36906946\n",
            "Iteration 1239, loss = 0.36852538\n",
            "Iteration 1240, loss = 0.36798189\n",
            "Iteration 1241, loss = 0.36743900\n",
            "Iteration 1242, loss = 0.36689671\n",
            "Iteration 1243, loss = 0.36635503\n",
            "Iteration 1244, loss = 0.36581395\n",
            "Iteration 1245, loss = 0.36527348\n",
            "Iteration 1246, loss = 0.36473362\n",
            "Iteration 1247, loss = 0.36419439\n",
            "Iteration 1248, loss = 0.36365577\n",
            "Iteration 1249, loss = 0.36311777\n",
            "Iteration 1250, loss = 0.36258041\n",
            "Iteration 1251, loss = 0.36204367\n",
            "Iteration 1252, loss = 0.36150757\n",
            "Iteration 1253, loss = 0.36097210\n",
            "Iteration 1254, loss = 0.36043727\n",
            "Iteration 1255, loss = 0.35990309\n",
            "Iteration 1256, loss = 0.35936955\n",
            "Iteration 1257, loss = 0.35883666\n",
            "Iteration 1258, loss = 0.35830443\n",
            "Iteration 1259, loss = 0.35777355\n",
            "Iteration 1260, loss = 0.35724456\n",
            "Iteration 1261, loss = 0.35671616\n",
            "Iteration 1262, loss = 0.35618861\n",
            "Iteration 1263, loss = 0.35566200\n",
            "Iteration 1264, loss = 0.35513623\n",
            "Iteration 1265, loss = 0.35461120\n",
            "Iteration 1266, loss = 0.35408722\n",
            "Iteration 1267, loss = 0.35356330\n",
            "Iteration 1268, loss = 0.35304052\n",
            "Iteration 1269, loss = 0.35251859\n",
            "Iteration 1270, loss = 0.35199751\n",
            "Iteration 1271, loss = 0.35147727\n",
            "Iteration 1272, loss = 0.35095789\n",
            "Iteration 1273, loss = 0.35043937\n",
            "Iteration 1274, loss = 0.34992172\n",
            "Iteration 1275, loss = 0.34940523\n",
            "Iteration 1276, loss = 0.34888933\n",
            "Iteration 1277, loss = 0.34837434\n",
            "Iteration 1278, loss = 0.34786015\n",
            "Iteration 1279, loss = 0.34734679\n",
            "Iteration 1280, loss = 0.34683429\n",
            "Iteration 1281, loss = 0.34632264\n",
            "Iteration 1282, loss = 0.34581185\n",
            "Iteration 1283, loss = 0.34530191\n",
            "Iteration 1284, loss = 0.34479281\n",
            "Iteration 1285, loss = 0.34428452\n",
            "Iteration 1286, loss = 0.34377704\n",
            "Iteration 1287, loss = 0.34327037\n",
            "Iteration 1288, loss = 0.34276450\n",
            "Iteration 1289, loss = 0.34225944\n",
            "Iteration 1290, loss = 0.34175519\n",
            "Iteration 1291, loss = 0.34125173\n",
            "Iteration 1292, loss = 0.34074907\n",
            "Iteration 1293, loss = 0.34024721\n",
            "Iteration 1294, loss = 0.33974613\n",
            "Iteration 1295, loss = 0.33924584\n",
            "Iteration 1296, loss = 0.33874634\n",
            "Iteration 1297, loss = 0.33824763\n",
            "Iteration 1298, loss = 0.33774971\n",
            "Iteration 1299, loss = 0.33725340\n",
            "Iteration 1300, loss = 0.33675718\n",
            "Iteration 1301, loss = 0.33626238\n",
            "Iteration 1302, loss = 0.33576821\n",
            "Iteration 1303, loss = 0.33527559\n",
            "Iteration 1304, loss = 0.33478304\n",
            "Iteration 1305, loss = 0.33429184\n",
            "Iteration 1306, loss = 0.33380147\n",
            "Iteration 1307, loss = 0.33331169\n",
            "Iteration 1308, loss = 0.33282286\n",
            "Iteration 1309, loss = 0.33233523\n",
            "Iteration 1310, loss = 0.33184808\n",
            "Iteration 1311, loss = 0.33136198\n",
            "Iteration 1312, loss = 0.33087693\n",
            "Iteration 1313, loss = 0.33039247\n",
            "Iteration 1314, loss = 0.32990925\n",
            "Iteration 1315, loss = 0.32942670\n",
            "Iteration 1316, loss = 0.32894497\n",
            "Iteration 1317, loss = 0.32846447\n",
            "Iteration 1318, loss = 0.32798463\n",
            "Iteration 1319, loss = 0.32750564\n",
            "Iteration 1320, loss = 0.32702753\n",
            "Iteration 1321, loss = 0.32655071\n",
            "Iteration 1322, loss = 0.32607437\n",
            "Iteration 1323, loss = 0.32559905\n",
            "Iteration 1324, loss = 0.32512461\n",
            "Iteration 1325, loss = 0.32465107\n",
            "Iteration 1326, loss = 0.32417846\n",
            "Iteration 1327, loss = 0.32370677\n",
            "Iteration 1328, loss = 0.32323601\n",
            "Iteration 1329, loss = 0.32276614\n",
            "Iteration 1330, loss = 0.32229719\n",
            "Iteration 1331, loss = 0.32182914\n",
            "Iteration 1332, loss = 0.32136199\n",
            "Iteration 1333, loss = 0.32089575\n",
            "Iteration 1334, loss = 0.32043041\n",
            "Iteration 1335, loss = 0.31996597\n",
            "Iteration 1336, loss = 0.31950243\n",
            "Iteration 1337, loss = 0.31903978\n",
            "Iteration 1338, loss = 0.31857802\n",
            "Iteration 1339, loss = 0.31811715\n",
            "Iteration 1340, loss = 0.31765716\n",
            "Iteration 1341, loss = 0.31719805\n",
            "Iteration 1342, loss = 0.31673982\n",
            "Iteration 1343, loss = 0.31628246\n",
            "Iteration 1344, loss = 0.31582598\n",
            "Iteration 1345, loss = 0.31537037\n",
            "Iteration 1346, loss = 0.31491563\n",
            "Iteration 1347, loss = 0.31446176\n",
            "Iteration 1348, loss = 0.31400875\n",
            "Iteration 1349, loss = 0.31355660\n",
            "Iteration 1350, loss = 0.31310533\n",
            "Iteration 1351, loss = 0.31265490\n",
            "Iteration 1352, loss = 0.31220535\n",
            "Iteration 1353, loss = 0.31175665\n",
            "Iteration 1354, loss = 0.31130881\n",
            "Iteration 1355, loss = 0.31086182\n",
            "Iteration 1356, loss = 0.31041570\n",
            "Iteration 1357, loss = 0.30997043\n",
            "Iteration 1358, loss = 0.30952601\n",
            "Iteration 1359, loss = 0.30908245\n",
            "Iteration 1360, loss = 0.30863974\n",
            "Iteration 1361, loss = 0.30819789\n",
            "Iteration 1362, loss = 0.30775689\n",
            "Iteration 1363, loss = 0.30731674\n",
            "Iteration 1364, loss = 0.30687744\n",
            "Iteration 1365, loss = 0.30643899\n",
            "Iteration 1366, loss = 0.30600139\n",
            "Iteration 1367, loss = 0.30556464\n",
            "Iteration 1368, loss = 0.30512874\n",
            "Iteration 1369, loss = 0.30469369\n",
            "Iteration 1370, loss = 0.30425948\n",
            "Iteration 1371, loss = 0.30382612\n",
            "Iteration 1372, loss = 0.30339361\n",
            "Iteration 1373, loss = 0.30296195\n",
            "Iteration 1374, loss = 0.30253113\n",
            "Iteration 1375, loss = 0.30210115\n",
            "Iteration 1376, loss = 0.30167202\n",
            "Iteration 1377, loss = 0.30124374\n",
            "Iteration 1378, loss = 0.30081630\n",
            "Iteration 1379, loss = 0.30038970\n",
            "Iteration 1380, loss = 0.29996394\n",
            "Iteration 1381, loss = 0.29953903\n",
            "Iteration 1382, loss = 0.29911497\n",
            "Iteration 1383, loss = 0.29869174\n",
            "Iteration 1384, loss = 0.29826936\n",
            "Iteration 1385, loss = 0.29784782\n",
            "Iteration 1386, loss = 0.29742712\n",
            "Iteration 1387, loss = 0.29700728\n",
            "Iteration 1388, loss = 0.29658832\n",
            "Iteration 1389, loss = 0.29617019\n",
            "Iteration 1390, loss = 0.29575290\n",
            "Iteration 1391, loss = 0.29533646\n",
            "Iteration 1392, loss = 0.29492088\n",
            "Iteration 1393, loss = 0.29450613\n",
            "Iteration 1394, loss = 0.29409223\n",
            "Iteration 1395, loss = 0.29367916\n",
            "Iteration 1396, loss = 0.29326693\n",
            "Iteration 1397, loss = 0.29285555\n",
            "Iteration 1398, loss = 0.29244500\n",
            "Iteration 1399, loss = 0.29203528\n",
            "Iteration 1400, loss = 0.29162641\n",
            "Iteration 1401, loss = 0.29121837\n",
            "Iteration 1402, loss = 0.29081118\n",
            "Iteration 1403, loss = 0.29040482\n",
            "Iteration 1404, loss = 0.28999930\n",
            "Iteration 1405, loss = 0.28959461\n",
            "Iteration 1406, loss = 0.28919077\n",
            "Iteration 1407, loss = 0.28878776\n",
            "Iteration 1408, loss = 0.28838559\n",
            "Iteration 1409, loss = 0.28798426\n",
            "Iteration 1410, loss = 0.28758376\n",
            "Iteration 1411, loss = 0.28718411\n",
            "Iteration 1412, loss = 0.28678529\n",
            "Iteration 1413, loss = 0.28638731\n",
            "Iteration 1414, loss = 0.28599016\n",
            "Iteration 1415, loss = 0.28559386\n",
            "Iteration 1416, loss = 0.28519840\n",
            "Iteration 1417, loss = 0.28480377\n",
            "Iteration 1418, loss = 0.28440998\n",
            "Iteration 1419, loss = 0.28401702\n",
            "Iteration 1420, loss = 0.28362490\n",
            "Iteration 1421, loss = 0.28323362\n",
            "Iteration 1422, loss = 0.28284317\n",
            "Iteration 1423, loss = 0.28245355\n",
            "Iteration 1424, loss = 0.28206477\n",
            "Iteration 1425, loss = 0.28167683\n",
            "Iteration 1426, loss = 0.28128972\n",
            "Iteration 1427, loss = 0.28090345\n",
            "Iteration 1428, loss = 0.28051801\n",
            "Iteration 1429, loss = 0.28013341\n",
            "Iteration 1430, loss = 0.27974964\n",
            "Iteration 1431, loss = 0.27936670\n",
            "Iteration 1432, loss = 0.27898459\n",
            "Iteration 1433, loss = 0.27860332\n",
            "Iteration 1434, loss = 0.27822355\n",
            "Iteration 1435, loss = 0.27784478\n",
            "Iteration 1436, loss = 0.27746689\n",
            "Iteration 1437, loss = 0.27708991\n",
            "Iteration 1438, loss = 0.27671385\n",
            "Iteration 1439, loss = 0.27633867\n",
            "Iteration 1440, loss = 0.27596434\n",
            "Iteration 1441, loss = 0.27559085\n",
            "Iteration 1442, loss = 0.27521821\n",
            "Iteration 1443, loss = 0.27484643\n",
            "Iteration 1444, loss = 0.27447551\n",
            "Iteration 1445, loss = 0.27410548\n",
            "Iteration 1446, loss = 0.27373632\n",
            "Iteration 1447, loss = 0.27336803\n",
            "Iteration 1448, loss = 0.27300061\n",
            "Iteration 1449, loss = 0.27263404\n",
            "Iteration 1450, loss = 0.27226834\n",
            "Iteration 1451, loss = 0.27190349\n",
            "Iteration 1452, loss = 0.27153949\n",
            "Iteration 1453, loss = 0.27117634\n",
            "Iteration 1454, loss = 0.27081404\n",
            "Iteration 1455, loss = 0.27045259\n",
            "Iteration 1456, loss = 0.27009197\n",
            "Iteration 1457, loss = 0.26973218\n",
            "Iteration 1458, loss = 0.26937323\n",
            "Iteration 1459, loss = 0.26901511\n",
            "Iteration 1460, loss = 0.26865781\n",
            "Iteration 1461, loss = 0.26830133\n",
            "Iteration 1462, loss = 0.26794568\n",
            "Iteration 1463, loss = 0.26759084\n",
            "Iteration 1464, loss = 0.26723682\n",
            "Iteration 1465, loss = 0.26688361\n",
            "Iteration 1466, loss = 0.26653122\n",
            "Iteration 1467, loss = 0.26617964\n",
            "Iteration 1468, loss = 0.26582887\n",
            "Iteration 1469, loss = 0.26547890\n",
            "Iteration 1470, loss = 0.26512975\n",
            "Iteration 1471, loss = 0.26478140\n",
            "Iteration 1472, loss = 0.26443385\n",
            "Iteration 1473, loss = 0.26408711\n",
            "Iteration 1474, loss = 0.26374117\n",
            "Iteration 1475, loss = 0.26339603\n",
            "Iteration 1476, loss = 0.26305169\n",
            "Iteration 1477, loss = 0.26270816\n",
            "Iteration 1478, loss = 0.26236542\n",
            "Iteration 1479, loss = 0.26202348\n",
            "Iteration 1480, loss = 0.26168234\n",
            "Iteration 1481, loss = 0.26134199\n",
            "Iteration 1482, loss = 0.26100244\n",
            "Iteration 1483, loss = 0.26066369\n",
            "Iteration 1484, loss = 0.26032573\n",
            "Iteration 1485, loss = 0.25998963\n",
            "Iteration 1486, loss = 0.25965422\n",
            "Iteration 1487, loss = 0.25931966\n",
            "Iteration 1488, loss = 0.25898600\n",
            "Iteration 1489, loss = 0.25865325\n",
            "Iteration 1490, loss = 0.25832135\n",
            "Iteration 1491, loss = 0.25799048\n",
            "Iteration 1492, loss = 0.25766030\n",
            "Iteration 1493, loss = 0.25733089\n",
            "Iteration 1494, loss = 0.25700231\n",
            "Iteration 1495, loss = 0.25667460\n",
            "Iteration 1496, loss = 0.25634778\n",
            "Iteration 1497, loss = 0.25602183\n",
            "Iteration 1498, loss = 0.25569674\n",
            "Iteration 1499, loss = 0.25537249\n",
            "Iteration 1500, loss = 0.25504907\n",
            "Iteration 1501, loss = 0.25472648\n",
            "Iteration 1502, loss = 0.25440471\n",
            "Iteration 1503, loss = 0.25408376\n",
            "Iteration 1504, loss = 0.25376362\n",
            "Iteration 1505, loss = 0.25344430\n",
            "Iteration 1506, loss = 0.25312579\n",
            "Iteration 1507, loss = 0.25280807\n",
            "Iteration 1508, loss = 0.25249114\n",
            "Iteration 1509, loss = 0.25217500\n",
            "Iteration 1510, loss = 0.25185965\n",
            "Iteration 1511, loss = 0.25154507\n",
            "Iteration 1512, loss = 0.25123127\n",
            "Iteration 1513, loss = 0.25091824\n",
            "Iteration 1514, loss = 0.25060598\n",
            "Iteration 1515, loss = 0.25029449\n",
            "Iteration 1516, loss = 0.24998375\n",
            "Iteration 1517, loss = 0.24967378\n",
            "Iteration 1518, loss = 0.24936456\n",
            "Iteration 1519, loss = 0.24905609\n",
            "Iteration 1520, loss = 0.24874838\n",
            "Iteration 1521, loss = 0.24844141\n",
            "Iteration 1522, loss = 0.24813519\n",
            "Iteration 1523, loss = 0.24782972\n",
            "Iteration 1524, loss = 0.24752499\n",
            "Iteration 1525, loss = 0.24722100\n",
            "Iteration 1526, loss = 0.24691775\n",
            "Iteration 1527, loss = 0.24661525\n",
            "Iteration 1528, loss = 0.24631348\n",
            "Iteration 1529, loss = 0.24601244\n",
            "Iteration 1530, loss = 0.24571214\n",
            "Iteration 1531, loss = 0.24541257\n",
            "Iteration 1532, loss = 0.24511373\n",
            "Iteration 1533, loss = 0.24481563\n",
            "Iteration 1534, loss = 0.24451825\n",
            "Iteration 1535, loss = 0.24422160\n",
            "Iteration 1536, loss = 0.24392567\n",
            "Iteration 1537, loss = 0.24363047\n",
            "Iteration 1538, loss = 0.24333599\n",
            "Iteration 1539, loss = 0.24304224\n",
            "Iteration 1540, loss = 0.24274920\n",
            "Iteration 1541, loss = 0.24245688\n",
            "Iteration 1542, loss = 0.24216528\n",
            "Iteration 1543, loss = 0.24187439\n",
            "Iteration 1544, loss = 0.24158421\n",
            "Iteration 1545, loss = 0.24129475\n",
            "Iteration 1546, loss = 0.24100600\n",
            "Iteration 1547, loss = 0.24071796\n",
            "Iteration 1548, loss = 0.24043063\n",
            "Iteration 1549, loss = 0.24014400\n",
            "Iteration 1550, loss = 0.23985808\n",
            "Iteration 1551, loss = 0.23957286\n",
            "Iteration 1552, loss = 0.23928834\n",
            "Iteration 1553, loss = 0.23900453\n",
            "Iteration 1554, loss = 0.23872141\n",
            "Iteration 1555, loss = 0.23843899\n",
            "Iteration 1556, loss = 0.23815727\n",
            "Iteration 1557, loss = 0.23787624\n",
            "Iteration 1558, loss = 0.23759591\n",
            "Iteration 1559, loss = 0.23731627\n",
            "Iteration 1560, loss = 0.23703732\n",
            "Iteration 1561, loss = 0.23675906\n",
            "Iteration 1562, loss = 0.23648148\n",
            "Iteration 1563, loss = 0.23620460\n",
            "Iteration 1564, loss = 0.23592840\n",
            "Iteration 1565, loss = 0.23565288\n",
            "Iteration 1566, loss = 0.23537805\n",
            "Iteration 1567, loss = 0.23510389\n",
            "Iteration 1568, loss = 0.23483042\n",
            "Iteration 1569, loss = 0.23455762\n",
            "Iteration 1570, loss = 0.23428551\n",
            "Iteration 1571, loss = 0.23401406\n",
            "Iteration 1572, loss = 0.23374329\n",
            "Iteration 1573, loss = 0.23347320\n",
            "Iteration 1574, loss = 0.23320378\n",
            "Iteration 1575, loss = 0.23293502\n",
            "Iteration 1576, loss = 0.23266694\n",
            "Iteration 1577, loss = 0.23239952\n",
            "Iteration 1578, loss = 0.23213277\n",
            "Iteration 1579, loss = 0.23186668\n",
            "Iteration 1580, loss = 0.23160126\n",
            "Iteration 1581, loss = 0.23133650\n",
            "Iteration 1582, loss = 0.23107240\n",
            "Iteration 1583, loss = 0.23080896\n",
            "Iteration 1584, loss = 0.23054618\n",
            "Iteration 1585, loss = 0.23028405\n",
            "Iteration 1586, loss = 0.23002258\n",
            "Iteration 1587, loss = 0.22976176\n",
            "Iteration 1588, loss = 0.22950159\n",
            "Iteration 1589, loss = 0.22924208\n",
            "Iteration 1590, loss = 0.22898322\n",
            "Iteration 1591, loss = 0.22872500\n",
            "Iteration 1592, loss = 0.22846743\n",
            "Iteration 1593, loss = 0.22821051\n",
            "Iteration 1594, loss = 0.22795423\n",
            "Iteration 1595, loss = 0.22769859\n",
            "Iteration 1596, loss = 0.22744360\n",
            "Iteration 1597, loss = 0.22718924\n",
            "Iteration 1598, loss = 0.22693553\n",
            "Iteration 1599, loss = 0.22668245\n",
            "Iteration 1600, loss = 0.22643001\n",
            "Iteration 1601, loss = 0.22617820\n",
            "Iteration 1602, loss = 0.22592703\n",
            "Iteration 1603, loss = 0.22567648\n",
            "Iteration 1604, loss = 0.22542657\n",
            "Iteration 1605, loss = 0.22517729\n",
            "Iteration 1606, loss = 0.22492864\n",
            "Iteration 1607, loss = 0.22468061\n",
            "Iteration 1608, loss = 0.22443320\n",
            "Iteration 1609, loss = 0.22418642\n",
            "Iteration 1610, loss = 0.22394027\n",
            "Iteration 1611, loss = 0.22369473\n",
            "Iteration 1612, loss = 0.22344982\n",
            "Iteration 1613, loss = 0.22320552\n",
            "Iteration 1614, loss = 0.22296184\n",
            "Iteration 1615, loss = 0.22271877\n",
            "Iteration 1616, loss = 0.22247632\n",
            "Iteration 1617, loss = 0.22223448\n",
            "Iteration 1618, loss = 0.22199325\n",
            "Iteration 1619, loss = 0.22175263\n",
            "Iteration 1620, loss = 0.22151262\n",
            "Iteration 1621, loss = 0.22127322\n",
            "Iteration 1622, loss = 0.22103442\n",
            "Iteration 1623, loss = 0.22079623\n",
            "Iteration 1624, loss = 0.22055864\n",
            "Iteration 1625, loss = 0.22032166\n",
            "Iteration 1626, loss = 0.22008527\n",
            "Iteration 1627, loss = 0.21984948\n",
            "Iteration 1628, loss = 0.21961429\n",
            "Iteration 1629, loss = 0.21937970\n",
            "Iteration 1630, loss = 0.21914570\n",
            "Iteration 1631, loss = 0.21891229\n",
            "Iteration 1632, loss = 0.21867948\n",
            "Iteration 1633, loss = 0.21844726\n",
            "Iteration 1634, loss = 0.21821562\n",
            "Iteration 1635, loss = 0.21798458\n",
            "Iteration 1636, loss = 0.21775412\n",
            "Iteration 1637, loss = 0.21752424\n",
            "Iteration 1638, loss = 0.21729495\n",
            "Iteration 1639, loss = 0.21706624\n",
            "Iteration 1640, loss = 0.21683811\n",
            "Iteration 1641, loss = 0.21661057\n",
            "Iteration 1642, loss = 0.21638439\n",
            "Iteration 1643, loss = 0.21615825\n",
            "Iteration 1644, loss = 0.21593270\n",
            "Iteration 1645, loss = 0.21570783\n",
            "Iteration 1646, loss = 0.21548363\n",
            "Iteration 1647, loss = 0.21526025\n",
            "Iteration 1648, loss = 0.21503702\n",
            "Iteration 1649, loss = 0.21481455\n",
            "Iteration 1650, loss = 0.21459263\n",
            "Iteration 1651, loss = 0.21437130\n",
            "Iteration 1652, loss = 0.21415055\n",
            "Iteration 1653, loss = 0.21393039\n",
            "Iteration 1654, loss = 0.21371102\n",
            "Iteration 1655, loss = 0.21349199\n",
            "Iteration 1656, loss = 0.21327364\n",
            "Iteration 1657, loss = 0.21305584\n",
            "Iteration 1658, loss = 0.21283879\n",
            "Iteration 1659, loss = 0.21262214\n",
            "Iteration 1660, loss = 0.21240608\n",
            "Iteration 1661, loss = 0.21219060\n",
            "Iteration 1662, loss = 0.21197583\n",
            "Iteration 1663, loss = 0.21176148\n",
            "Iteration 1664, loss = 0.21154769\n",
            "Iteration 1665, loss = 0.21133447\n",
            "Iteration 1666, loss = 0.21112184\n",
            "Iteration 1667, loss = 0.21090982\n",
            "Iteration 1668, loss = 0.21069829\n",
            "Iteration 1669, loss = 0.21048734\n",
            "Iteration 1670, loss = 0.21027694\n",
            "Iteration 1671, loss = 0.21006707\n",
            "Iteration 1672, loss = 0.20985776\n",
            "Iteration 1673, loss = 0.20964899\n",
            "Iteration 1674, loss = 0.20944076\n",
            "Iteration 1675, loss = 0.20923308\n",
            "Iteration 1676, loss = 0.20902594\n",
            "Iteration 1677, loss = 0.20881933\n",
            "Iteration 1678, loss = 0.20861326\n",
            "Iteration 1679, loss = 0.20840772\n",
            "Iteration 1680, loss = 0.20820271\n",
            "Iteration 1681, loss = 0.20799823\n",
            "Iteration 1682, loss = 0.20779428\n",
            "Iteration 1683, loss = 0.20759085\n",
            "Iteration 1684, loss = 0.20738794\n",
            "Iteration 1685, loss = 0.20718555\n",
            "Iteration 1686, loss = 0.20698368\n",
            "Iteration 1687, loss = 0.20678233\n",
            "Iteration 1688, loss = 0.20658149\n",
            "Iteration 1689, loss = 0.20638117\n",
            "Iteration 1690, loss = 0.20618135\n",
            "Iteration 1691, loss = 0.20598205\n",
            "Iteration 1692, loss = 0.20578325\n",
            "Iteration 1693, loss = 0.20558496\n",
            "Iteration 1694, loss = 0.20538717\n",
            "Iteration 1695, loss = 0.20518989\n",
            "Iteration 1696, loss = 0.20499311\n",
            "Iteration 1697, loss = 0.20479683\n",
            "Iteration 1698, loss = 0.20460104\n",
            "Iteration 1699, loss = 0.20440576\n",
            "Iteration 1700, loss = 0.20421097\n",
            "Iteration 1701, loss = 0.20401667\n",
            "Iteration 1702, loss = 0.20382287\n",
            "Iteration 1703, loss = 0.20362956\n",
            "Iteration 1704, loss = 0.20343673\n",
            "Iteration 1705, loss = 0.20324440\n",
            "Iteration 1706, loss = 0.20305255\n",
            "Iteration 1707, loss = 0.20286119\n",
            "Iteration 1708, loss = 0.20267031\n",
            "Iteration 1709, loss = 0.20247992\n",
            "Iteration 1710, loss = 0.20229001\n",
            "Iteration 1711, loss = 0.20210057\n",
            "Iteration 1712, loss = 0.20191162\n",
            "Iteration 1713, loss = 0.20172314\n",
            "Iteration 1714, loss = 0.20153514\n",
            "Iteration 1715, loss = 0.20134761\n",
            "Iteration 1716, loss = 0.20116056\n",
            "Iteration 1717, loss = 0.20097398\n",
            "Iteration 1718, loss = 0.20078786\n",
            "Iteration 1719, loss = 0.20060222\n",
            "Iteration 1720, loss = 0.20041705\n",
            "Iteration 1721, loss = 0.20023234\n",
            "Iteration 1722, loss = 0.20004810\n",
            "Iteration 1723, loss = 0.19986432\n",
            "Iteration 1724, loss = 0.19968100\n",
            "Iteration 1725, loss = 0.19949814\n",
            "Iteration 1726, loss = 0.19931575\n",
            "Iteration 1727, loss = 0.19913381\n",
            "Iteration 1728, loss = 0.19895233\n",
            "Iteration 1729, loss = 0.19877130\n",
            "Iteration 1730, loss = 0.19859073\n",
            "Iteration 1731, loss = 0.19841062\n",
            "Iteration 1732, loss = 0.19823095\n",
            "Iteration 1733, loss = 0.19805173\n",
            "Iteration 1734, loss = 0.19787297\n",
            "Iteration 1735, loss = 0.19769465\n",
            "Iteration 1736, loss = 0.19751678\n",
            "Iteration 1737, loss = 0.19733936\n",
            "Iteration 1738, loss = 0.19716237\n",
            "Iteration 1739, loss = 0.19698584\n",
            "Iteration 1740, loss = 0.19680974\n",
            "Iteration 1741, loss = 0.19663408\n",
            "Iteration 1742, loss = 0.19645887\n",
            "Iteration 1743, loss = 0.19628409\n",
            "Iteration 1744, loss = 0.19610974\n",
            "Iteration 1745, loss = 0.19593584\n",
            "Iteration 1746, loss = 0.19576236\n",
            "Iteration 1747, loss = 0.19558932\n",
            "Iteration 1748, loss = 0.19541671\n",
            "Iteration 1749, loss = 0.19524454\n",
            "Iteration 1750, loss = 0.19507279\n",
            "Iteration 1751, loss = 0.19490147\n",
            "Iteration 1752, loss = 0.19473057\n",
            "Iteration 1753, loss = 0.19456010\n",
            "Iteration 1754, loss = 0.19439006\n",
            "Iteration 1755, loss = 0.19422043\n",
            "Iteration 1756, loss = 0.19405123\n",
            "Iteration 1757, loss = 0.19388246\n",
            "Iteration 1758, loss = 0.19371410\n",
            "Iteration 1759, loss = 0.19354615\n",
            "Iteration 1760, loss = 0.19337863\n",
            "Iteration 1761, loss = 0.19321152\n",
            "Iteration 1762, loss = 0.19304483\n",
            "Iteration 1763, loss = 0.19287854\n",
            "Iteration 1764, loss = 0.19271268\n",
            "Iteration 1765, loss = 0.19254722\n",
            "Iteration 1766, loss = 0.19238217\n",
            "Iteration 1767, loss = 0.19221753\n",
            "Iteration 1768, loss = 0.19205330\n",
            "Iteration 1769, loss = 0.19188947\n",
            "Iteration 1770, loss = 0.19172605\n",
            "Iteration 1771, loss = 0.19156304\n",
            "Iteration 1772, loss = 0.19140042\n",
            "Iteration 1773, loss = 0.19123821\n",
            "Iteration 1774, loss = 0.19107640\n",
            "Iteration 1775, loss = 0.19091499\n",
            "Iteration 1776, loss = 0.19075398\n",
            "Iteration 1777, loss = 0.19059336\n",
            "Iteration 1778, loss = 0.19043314\n",
            "Iteration 1779, loss = 0.19027331\n",
            "Iteration 1780, loss = 0.19011388\n",
            "Iteration 1781, loss = 0.18995484\n",
            "Iteration 1782, loss = 0.18979619\n",
            "Iteration 1783, loss = 0.18963794\n",
            "Iteration 1784, loss = 0.18948007\n",
            "Iteration 1785, loss = 0.18932259\n",
            "Iteration 1786, loss = 0.18916549\n",
            "Iteration 1787, loss = 0.18900878\n",
            "Iteration 1788, loss = 0.18885246\n",
            "Iteration 1789, loss = 0.18869652\n",
            "Iteration 1790, loss = 0.18854096\n",
            "Iteration 1791, loss = 0.18838579\n",
            "Iteration 1792, loss = 0.18823099\n",
            "Iteration 1793, loss = 0.18807657\n",
            "Iteration 1794, loss = 0.18792253\n",
            "Iteration 1795, loss = 0.18776887\n",
            "Iteration 1796, loss = 0.18761558\n",
            "Iteration 1797, loss = 0.18746267\n",
            "Iteration 1798, loss = 0.18731013\n",
            "Iteration 1799, loss = 0.18715797\n",
            "Iteration 1800, loss = 0.18700617\n",
            "Iteration 1801, loss = 0.18685475\n",
            "Iteration 1802, loss = 0.18670369\n",
            "Iteration 1803, loss = 0.18655301\n",
            "Iteration 1804, loss = 0.18640269\n",
            "Iteration 1805, loss = 0.18625273\n",
            "Iteration 1806, loss = 0.18610314\n",
            "Iteration 1807, loss = 0.18595392\n",
            "Iteration 1808, loss = 0.18580506\n",
            "Iteration 1809, loss = 0.18565656\n",
            "Iteration 1810, loss = 0.18550842\n",
            "Iteration 1811, loss = 0.18536064\n",
            "Iteration 1812, loss = 0.18521322\n",
            "Iteration 1813, loss = 0.18506615\n",
            "Iteration 1814, loss = 0.18491945\n",
            "Iteration 1815, loss = 0.18477310\n",
            "Iteration 1816, loss = 0.18462710\n",
            "Iteration 1817, loss = 0.18448146\n",
            "Iteration 1818, loss = 0.18433616\n",
            "Iteration 1819, loss = 0.18419122\n",
            "Iteration 1820, loss = 0.18404664\n",
            "Iteration 1821, loss = 0.18390240\n",
            "Iteration 1822, loss = 0.18375850\n",
            "Iteration 1823, loss = 0.18361496\n",
            "Iteration 1824, loss = 0.18347176\n",
            "Iteration 1825, loss = 0.18332891\n",
            "Iteration 1826, loss = 0.18318640\n",
            "Iteration 1827, loss = 0.18304423\n",
            "Iteration 1828, loss = 0.18290241\n",
            "Iteration 1829, loss = 0.18276093\n",
            "Iteration 1830, loss = 0.18261978\n",
            "Iteration 1831, loss = 0.18247898\n",
            "Iteration 1832, loss = 0.18233852\n",
            "Iteration 1833, loss = 0.18219839\n",
            "Iteration 1834, loss = 0.18205860\n",
            "Iteration 1835, loss = 0.18191914\n",
            "Iteration 1836, loss = 0.18178002\n",
            "Iteration 1837, loss = 0.18164123\n",
            "Iteration 1838, loss = 0.18150277\n",
            "Iteration 1839, loss = 0.18136465\n",
            "Iteration 1840, loss = 0.18122685\n",
            "Iteration 1841, loss = 0.18108939\n",
            "Iteration 1842, loss = 0.18095225\n",
            "Iteration 1843, loss = 0.18081544\n",
            "Iteration 1844, loss = 0.18067896\n",
            "Iteration 1845, loss = 0.18054280\n",
            "Iteration 1846, loss = 0.18040697\n",
            "Iteration 1847, loss = 0.18027146\n",
            "Iteration 1848, loss = 0.18013627\n",
            "Iteration 1849, loss = 0.18000162\n",
            "Iteration 1850, loss = 0.17986707\n",
            "Iteration 1851, loss = 0.17973292\n",
            "Iteration 1852, loss = 0.17959915\n",
            "Iteration 1853, loss = 0.17946579\n",
            "Iteration 1854, loss = 0.17933267\n",
            "Iteration 1855, loss = 0.17919995\n",
            "Iteration 1856, loss = 0.17906759\n",
            "Iteration 1857, loss = 0.17893553\n",
            "Iteration 1858, loss = 0.17880379\n",
            "Iteration 1859, loss = 0.17867239\n",
            "Iteration 1860, loss = 0.17854135\n",
            "Iteration 1861, loss = 0.17841062\n",
            "Iteration 1862, loss = 0.17828020\n",
            "Iteration 1863, loss = 0.17815009\n",
            "Iteration 1864, loss = 0.17802029\n",
            "Iteration 1865, loss = 0.17789081\n",
            "Iteration 1866, loss = 0.17776163\n",
            "Iteration 1867, loss = 0.17763276\n",
            "Iteration 1868, loss = 0.17750419\n",
            "Iteration 1869, loss = 0.17737593\n",
            "Iteration 1870, loss = 0.17724797\n",
            "Iteration 1871, loss = 0.17712031\n",
            "Iteration 1872, loss = 0.17699294\n",
            "Iteration 1873, loss = 0.17686588\n",
            "Iteration 1874, loss = 0.17673911\n",
            "Iteration 1875, loss = 0.17661264\n",
            "Iteration 1876, loss = 0.17648647\n",
            "Iteration 1877, loss = 0.17636059\n",
            "Iteration 1878, loss = 0.17623501\n",
            "Iteration 1879, loss = 0.17610972\n",
            "Iteration 1880, loss = 0.17598473\n",
            "Iteration 1881, loss = 0.17586002\n",
            "Iteration 1882, loss = 0.17573561\n",
            "Iteration 1883, loss = 0.17561148\n",
            "Iteration 1884, loss = 0.17548764\n",
            "Iteration 1885, loss = 0.17536409\n",
            "Iteration 1886, loss = 0.17524083\n",
            "Iteration 1887, loss = 0.17511785\n",
            "Iteration 1888, loss = 0.17499516\n",
            "Iteration 1889, loss = 0.17487275\n",
            "Iteration 1890, loss = 0.17475063\n",
            "Iteration 1891, loss = 0.17462878\n",
            "Iteration 1892, loss = 0.17450722\n",
            "Iteration 1893, loss = 0.17438594\n",
            "Iteration 1894, loss = 0.17426494\n",
            "Iteration 1895, loss = 0.17414422\n",
            "Iteration 1896, loss = 0.17402378\n",
            "Iteration 1897, loss = 0.17390361\n",
            "Iteration 1898, loss = 0.17378372\n",
            "Iteration 1899, loss = 0.17366410\n",
            "Iteration 1900, loss = 0.17354476\n",
            "Iteration 1901, loss = 0.17342570\n",
            "Iteration 1902, loss = 0.17330691\n",
            "Iteration 1903, loss = 0.17318838\n",
            "Iteration 1904, loss = 0.17307014\n",
            "Iteration 1905, loss = 0.17295216\n",
            "Iteration 1906, loss = 0.17283445\n",
            "Iteration 1907, loss = 0.17271701\n",
            "Iteration 1908, loss = 0.17259984\n",
            "Iteration 1909, loss = 0.17248293\n",
            "Iteration 1910, loss = 0.17236629\n",
            "Iteration 1911, loss = 0.17224992\n",
            "Iteration 1912, loss = 0.17213381\n",
            "Iteration 1913, loss = 0.17201797\n",
            "Iteration 1914, loss = 0.17190239\n",
            "Iteration 1915, loss = 0.17178708\n",
            "Iteration 1916, loss = 0.17167202\n",
            "Iteration 1917, loss = 0.17155723\n",
            "Iteration 1918, loss = 0.17144269\n",
            "Iteration 1919, loss = 0.17132842\n",
            "Iteration 1920, loss = 0.17121440\n",
            "Iteration 1921, loss = 0.17110065\n",
            "Iteration 1922, loss = 0.17098715\n",
            "Iteration 1923, loss = 0.17087390\n",
            "Iteration 1924, loss = 0.17076091\n",
            "Iteration 1925, loss = 0.17064818\n",
            "Iteration 1926, loss = 0.17053570\n",
            "Iteration 1927, loss = 0.17042347\n",
            "Iteration 1928, loss = 0.17031150\n",
            "Iteration 1929, loss = 0.17019978\n",
            "Iteration 1930, loss = 0.17008831\n",
            "Iteration 1931, loss = 0.16997709\n",
            "Iteration 1932, loss = 0.16986612\n",
            "Iteration 1933, loss = 0.16975539\n",
            "Iteration 1934, loss = 0.16964492\n",
            "Iteration 1935, loss = 0.16953469\n",
            "Iteration 1936, loss = 0.16942471\n",
            "Iteration 1937, loss = 0.16931498\n",
            "Iteration 1938, loss = 0.16920549\n",
            "Iteration 1939, loss = 0.16909625\n",
            "Iteration 1940, loss = 0.16898727\n",
            "Iteration 1941, loss = 0.16887854\n",
            "Iteration 1942, loss = 0.16877006\n",
            "Iteration 1943, loss = 0.16866181\n",
            "Iteration 1944, loss = 0.16855381\n",
            "Iteration 1945, loss = 0.16844606\n",
            "Iteration 1946, loss = 0.16833854\n",
            "Iteration 1947, loss = 0.16823127\n",
            "Iteration 1948, loss = 0.16812423\n",
            "Iteration 1949, loss = 0.16801744\n",
            "Iteration 1950, loss = 0.16791088\n",
            "Iteration 1951, loss = 0.16780456\n",
            "Iteration 1952, loss = 0.16769847\n",
            "Iteration 1953, loss = 0.16759262\n",
            "Iteration 1954, loss = 0.16748700\n",
            "Iteration 1955, loss = 0.16738162\n",
            "Iteration 1956, loss = 0.16727646\n",
            "Iteration 1957, loss = 0.16717154\n",
            "Iteration 1958, loss = 0.16706686\n",
            "Iteration 1959, loss = 0.16696240\n",
            "Iteration 1960, loss = 0.16685817\n",
            "Iteration 1961, loss = 0.16675417\n",
            "Iteration 1962, loss = 0.16665040\n",
            "Iteration 1963, loss = 0.16654686\n",
            "Iteration 1964, loss = 0.16644354\n",
            "Iteration 1965, loss = 0.16634046\n",
            "Iteration 1966, loss = 0.16623759\n",
            "Iteration 1967, loss = 0.16613495\n",
            "Iteration 1968, loss = 0.16603254\n",
            "Iteration 1969, loss = 0.16593035\n",
            "Iteration 1970, loss = 0.16582838\n",
            "Iteration 1971, loss = 0.16572664\n",
            "Iteration 1972, loss = 0.16562511\n",
            "Iteration 1973, loss = 0.16552381\n",
            "Iteration 1974, loss = 0.16542273\n",
            "Iteration 1975, loss = 0.16532186\n",
            "Iteration 1976, loss = 0.16522122\n",
            "Iteration 1977, loss = 0.16512079\n",
            "Iteration 1978, loss = 0.16502059\n",
            "Iteration 1979, loss = 0.16492059\n",
            "Iteration 1980, loss = 0.16482082\n",
            "Iteration 1981, loss = 0.16472126\n",
            "Iteration 1982, loss = 0.16462192\n",
            "Iteration 1983, loss = 0.16452279\n",
            "Iteration 1984, loss = 0.16442387\n",
            "Iteration 1985, loss = 0.16432517\n",
            "Iteration 1986, loss = 0.16422668\n",
            "Iteration 1987, loss = 0.16412840\n",
            "Iteration 1988, loss = 0.16403033\n",
            "Iteration 1989, loss = 0.16393248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BCJhj8854_L"
      },
      "source": [
        "After finishing the training process we can use our trained machine learning by using model.predict() method. To get our classification result we can import classification_report from sklearn.matrix and call classification_report(real_target, prediction). To show results in confusion matrix and accuracy you also need to import them from sklearn.matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVh6hIw65-53",
        "outputId": "e161b071-b2f2-4d8c-9fbd-990775ef402f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#scikit for machine learning reporting\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test,y_pred)) # Print summary report\n",
        "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))) # Print Confusion matrix \n",
        "print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       0.81      0.93      0.87        14\n",
            "           2       1.00      0.82      0.90        17\n",
            "\n",
            "   micro avg       0.93      0.91      0.92        45\n",
            "   macro avg       0.94      0.92      0.92        45\n",
            "weighted avg       0.94      0.91      0.92        45\n",
            " samples avg       0.91      0.91      0.91        45\n",
            "\n",
            "[[14  0  0]\n",
            " [ 1 13  0]\n",
            " [ 0  3 14]]\n",
            "accuracy is  0.9111111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIHcEzk6Fzu"
      },
      "source": [
        "Last, if we want to show our machine learning loss history we can plot loss_curve_ property of our trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPqHB26z6HA8",
        "outputId": "2587a546-6fb5-470a-aa28-b836bcd232c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4942fc52b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcn+0oSkkCAAGHfERQVRQR30G7OtNXWrduoHftoO7YztdPqaKf9tZ1xOg5q62gX19Z2WtvRqUitoqCigogQ9rAHshHIRvbk+/vjnsQQkpCQm5x7b97Px+M+cu+5557zuefevPPN9yxfc84hIiLhL8rvAkREJDgU6CIiEUKBLiISIRToIiIRQoEuIhIhFOgiIhFCgS5yGmb2iJnd7XcdIqejQJeQYGb7zexyH9b7uJl9v9O0PDNzZhYD4Jy73Tn3r71Yli/vQaSNAl0kBLT98RDpDwW6hDQzizezB8zsiHd7wMziveeyzOz/zKzCzI6Z2Vozi/Ke+5aZHTazajPbaWaX9aOG9lZ8d+s0s6eAccALZlZjZv/kzf8xM9vqzf+amc3osNz9Xp2bgRNm9o9m9odO615hZv91prXL0KJWgYS67wALgXmAA/4X+C5wN/ANoBDI9uZdCDgzmwZ8BTjXOXfEzPKA6CDV0+U6nXM3mdli4EvOub8CmNlU4DfAJ4DXgH8gEPgznXON3us/A1wDHAXSgXvNLN05V+G12q8HlgepdolwaqFLqLsB+J5zrtQ5VwbcB9zkPdcEjALGO+eanHNrXeDiRC1APDDTzGKdc/udc3t6WMc3vRZ0hZlVAJt7mLe7dXblOuDPzrmXnXNNwP1AInBhh3lWOOcOOefqnHNFwBrgU95zy4Cjzrn3eqhHpJ0CXULdaOBAh8cHvGkA/w4UAH8xs71mdheAc64A+DpwL1BqZs+a2Wi6d79zLr3tBsztYd4u19mb2p1zrcAhYEyHeQ51es0TwI3e/RuBp3pYvshJFOgS6o4A4zs8HudNwzlX7Zz7hnNuIvAx4M62vnLn3K+dcxd5r3XAj4NRTE/r9NbTbe1mZsBY4HDHRXZ6zZ+AuWY2G/gI8Eww6pahQYEuoSTWzBI63GII9EF/18yyzSwLuAd4GsDMPmJmk72grCTQ1dJqZtPM7FJv52k9UAe0BqPA7tbpPV0CTOww+++Aa8zsMjOLJdD/3gC81d3ynXP1wO+BXwPvOucOBqNuGRoU6BJKXiQQvm23e4HvAxsI9GtvATZ60wCmAH8FaoB1wE+dc6sJ9J//iMCOxmJgBPDtINXY3ToBfkjgj0+FmX3TObeTQLfJg14tHwU+2mGHaHeeAOag7hbpI9MAFyKhxczGATuAHOdcld/1SPhQC10khHjH0d8JPKswl77ScegiIcLMkgn0wx8gcMiiSJ+oy0VEJEKoy0VEJEL41uWSlZXl8vLy/Fq9iEhYeu+9944657K7es63QM/Ly2PDhg1+rV5EJCyZ2YHunlOXi4hIhFCgi4hECAW6iEiE0HHoIhJ2mpqaKCwspL6+3u9SBkxCQgK5ubnExsb2+jUKdBEJO4WFhaSmppKXl0fgOmmRxTlHeXk5hYWFTJgwodevU5eLiISd+vp6MjMzIzLMAcyMzMzMPv8HokAXkbAUqWHe5kze32kD3czGmtlqM9vmDXb7tS7mWWpmlWa2ybvd0+dKemlncTX3r9pJeU3DQK1CRCQs9aaF3gx8wzk3k8CAuHeY2cwu5lvrnJvn3b4X1Co72FtWw0OrCyitVqCLiH9SUlL8LuEUpw1051yRc26jd78a2M7JYyIOqsS4wODttY0tfpUgIhKS+tSHbmZ5wHzgnS6evsDMPjCzlWY2q5vX32pmG8xsQ1lZWZ+LBUiKCxyYU6dAF5EQs2nTJhYuXMjcuXO59tprOX78OAArVqxg5syZzJ07l+uvvx6A119/nXnz5jFv3jzmz59PdXV1v9ff68MWzSwF+APw9S4uvL8RGO+cqzGzqwkMdDul8zKcc48CjwIsWLDgjK7bm9TeQm8+k5eLSIS574WtbDsS3LFAZo4exr98tMt2aY9uvvlmHnzwQZYsWcI999zDfffdxwMPPMCPfvQj9u3bR3x8PBUVFQDcf//9PPzwwyxatIiamhoSEhL6XXevWujeALd/AJ5xzj3X+XnnXJVzrsa7/yKBwX6z+l1dF9q6XOqa1EIXkdBRWVlJRUUFS5YsAeCWW25hzZo1AMydO5cbbriBp59+mpiYQDt60aJF3HnnnaxYsYKKior26f1x2iV4o5v/AtjunPtJN/PkACXOOWdm5xH4Q1He7+q6kOx1uagPXUSAM2pJD7Y///nPrFmzhhdeeIEf/OAHbNmyhbvuuotrrrmGF198kUWLFrFq1SqmT5/er/X05k/CIuAmYIuZbfKm/TMwDsA59wjwSeDLZtZMYLT2690ADYWknaIiEorS0tLIyMhg7dq1LF68mKeeeoolS5bQ2trKoUOHuOSSS7jooot49tlnqampoby8nDlz5jBnzhzWr1/Pjh07Bj7QnXNvAD0e4e6cewh4qF+V9FJbH3qd+tBFxEe1tbXk5ua2P77zzjt54oknuP3226mtrWXixIn86le/oqWlhRtvvJHKykqcc3z1q18lPT2du+++m9WrVxMVFcWsWbNYvnx5v2sKu2u5xEZHER1l1De1+l2KiAxhra1dZ9Dbb799yrQ33njjlGkPPvhg0GsKy1P/E2KitFNURKSTsAz0xLho6hXoIiInCctAj4+JVpeLyBA3QMddhIwzeX9hGegJsVFqoYsMYQkJCZSXl0dsqLddD72vJxuF3U5RUJeLyFCXm5tLYWEhZ3oJkXDQNmJRX4RloCfERFPfrEAXGapiY2P7NJLPUBGmXS7RujiXiEgnYRvo2ikqInKyMA30KHW5iIh0EqaBHk29ulxERE4SloGeGBtNfbO6XEREOgrLQE+IjdJOURGRTsI00AOHLUbqSQUiImcibAPdOWhsUbeLiEibsA10gPpGBbqISJswDfRA2Tp0UUTkQ+EZ6DFeC13XcxERaReWgd42rqgGuRAR+VBYBnp7l4tO/xcRaReega4uFxGRU4RnoKvLRUTkFOEZ6F4LvUGBLiLSLjwDXX3oIiKnCMtA11EuIiKnCstA105REZFThWegx6qFLiLSWVgGenyM+tBFRDoLy0CPijLiY6J0lIuISAdhGegQ6HZRl4uIyIfCNtATY6O1U1REpIOwDfSE2Cj1oYuIdBDGga4uFxGRjk4b6GY21sxWm9k2M9tqZl/rYh4zsxVmVmBmm83s7IEp90Mp8THU1DcP9GpERMJGb1rozcA3nHMzgYXAHWY2s9M8y4Ep3u1W4GdBrbILaYmxVNY1DfRqRETCxmkD3TlX5Jzb6N2vBrYDYzrN9nHgSRfwNpBuZqOCXm0HCnQRkZP1qQ/dzPKA+cA7nZ4aAxzq8LiQU0MfM7vVzDaY2YaysrK+VdrJsMRYqhToIiLteh3oZpYC/AH4unOu6kxW5px71Dm3wDm3IDs7+0wW0S4tMZbqhmZaWl2/liMiEil6FehmFksgzJ9xzj3XxSyHgbEdHud60wZMWmIsgFrpIiKe3hzlYsAvgO3OuZ90M9vzwM3e0S4LgUrnXFEQ6zxFW6BXKNBFRACI6cU8i4CbgC1mtsmb9s/AOADn3CPAi8DVQAFQC3w++KWebOSwBACKK+uZkJU80KsTEQl5pw1059wbgJ1mHgfcEayiemNMRiIAhyvqBnO1IiIhK2zPFB2dnkBstLG7tNrvUkREQkLYBnp8TDTzx2awbk+536WIiISEsA10gEtnjGBzYSW7StRKFxEJ60C/bsFY4mOi+NWb+/wuRUTEd2Ed6BnJcfzN2WN4buNhjp9o9LscERFfhXWgA3zuwgk0NLfy63cP+l2KiIivwj7Qp+WkctHkLJ5ct5+mFg14ISJDV9gHOsDnLsyjpKqBl7eV+F2KiIhvIiLQL5k+gjHpiTz99gG/SxER8U1EBHp0lPHZ88fx1p5yCkpr/C5HRMQXERHoAJ9eMJbYaOOZd9RKF5GhKWICPTs1nmWzR/H7DYU6hFFEhqSICXSAr1wymZrGZn76WoHfpYiIDLqICvRpOal88uxcHn9rvy4HICJDTkQFOsBdy6eTEh/Dt5/bQquGpxORISTiAj0zJZ7vXDOT9w4c5zfrdfaoiAwdERfoAH979hgunJTJj17cQUlVvd/liIgMiogMdDPj/107h8aWVu59fqvf5YiIDIqIDHSAvKxkvnb5FFbmF7Nqa7Hf5YiIDLiIDXSAv1s8kRmjhnHP/+ZTVd/kdzkiIgMqogM9NjqKH//tHMqqG/jxyh1+lyMiMqAiOtAB5uam84VFE3jmnYO8u++Y3+WIiAyYiA90gDuvnEpuRiJ3/ymfZl0zXUQi1JAI9KS4GL57zQx2llTz7PpDfpcjIjIghkSgA1w1K4fzJwznJy/vorJOO0hFJPIMmUA3M+756EyO1zay4pXdfpcjIhJ0QybQAWaNTuNT5+Ty1LoDHKmo87scEZGgGlKBDvDVy6bgcDy8WpfYFZHIMuQCPTcjievOHcvvNhzi0LFav8sREQmaIRfoAHdcMhkz48FX1ZcuIpFjSAb6qLREPnveOP6w8bBa6SISMYZkoAPctmQiUQaPvL7H71JERIJiyAb6qLREPnlOLv+zoZBSXTNdRCLAaQPdzH5pZqVmlt/N80vNrNLMNnm3e4Jf5sC47eJJNLe28tjavX6XIiLSb71poT8OLDvNPGudc/O82/f6X9bgyMtK5qNnjeaZdw5y/ESj3+WIiPTLaQPdObcGiNjLFP790snUNrbwqzf3+V2KiEi/BKsP/QIz+8DMVprZrO5mMrNbzWyDmW0oKysL0qr7Z1pOKlfOHMnjb+2nWoNgiEgYC0agbwTGO+fOAh4E/tTdjM65R51zC5xzC7Kzs4Ow6uD4yqWTqapv5qm3D/hdiojIGet3oDvnqpxzNd79F4FYM8vqd2WDaG5uOounZPGLtfuoa2zxuxwRkTPS70A3sxwzM+/+ed4yy/u73MH2lUsmU36ikWfXH/S7FBGRM9KbwxZ/A6wDpplZoZl90cxuN7PbvVk+CeSb2QfACuB655wbuJIHxvkTMzk3L4NH1+ylsVmjGolI+Ik53QzOuc+c5vmHgIeCVpGP7rhkMp/71Xr++H4h1507zu9yRET6ZMieKdqVJVOzmT1mGD97bY/GHhWRsKNA78DMuGPpZPaX1/JifrHf5YiI9IkCvZOrZuUwKTuZn64uIAx3BYjIEKZA7yQqyvjy0snsKK5m9c5Sv8sREek1BXoXPj5vNGPSE3noVbXSRSR8KNC7EBsdxW1LJrLxYAXv7IvYy9iISIRRoHfj0wvGkpUSx09f0wAYIhIeFOjdSIiN5gsXTWDNrjK2FFb6XY6IyGkp0Htw48LxpCbE8NPXCvwuRUTktBToPRiWEMstF+Tx0tZiCkpr/C5HRKRHCvTT+PyiPOJjojSYtIiEPAX6aWSmxPOZ88bxp/cPU3i81u9yRES6pUDvhb9bPBEzeGyNBpMWkdClQO+F0emJXDt/DM+uP0RZdYPf5YiIdEmB3ku3L5lEY0urBpMWkZClQO+lidkpXD1nFE+tO0BlnQaTFpHQo0Dvg79fOonqhmae1mDSIhKCFOh9MGt0GkunZfPLNzSYtIiEHgV6H93hDSb963c1mLSIhBYFeh+dmzecRZMzeXh1AdX16ksXkdChQD8D31o2nWMnGnlsrY54EZHQoUA/A3Nz07lmzih+vnavjksXkZChQD9D37hyKg3NrTz46m6/SxERARToZ2xidgrXnzuWX79zUFdiFJGQoEDvh3+4YiqJcdHc+/xWjT0qIr5ToPdDVko837hiKm8UHGVlfrHf5YjIEKdA76cbF45nek4q3/+/bdQ2NvtdjogMYQr0foqJjuJfPzGbI5X13L9ql9/liMgQpkAPgnPzhnPTwvH86q19vLO33O9yRGSIUqAHyV3LpzM2I4l//P1mTjSo60VEBp8CPUiS42O4/1Nnceh4Lfe9sNXvckRkCFKgB9F5E4bzlUsm87sNhfxuwyG/yxGRIUaBHmRfv3wqF07K5O4/5bPtSJXf5YjIEHLaQDezX5pZqZnld/O8mdkKMysws81mdnbwywwf0VHGis/MJz0pltue3sDRGl3rRUQGR29a6I8Dy3p4fjkwxbvdCvys/2WFt6yUeB658RzKqhv44uPrdXy6iAyK0wa6c24NcKyHWT4OPOkC3gbSzWxUsAoMV/PHZbDi+vlsPlzJV3/zPs0trX6XJCIRLhh96GOAjnsAC71ppzCzW81sg5ltKCsrC8KqQ9uVs3K472Oz+Ov2Ur72200KdREZUDGDuTLn3KPAowALFiwYElezuvmCPOoaW/jhyh0Y8MB184iJ1r5oEQm+YAT6YWBsh8e53jTx3LZkEgA/XLmDhuZWVlw/n8S4aJ+rEpFIE4ym4vPAzd7RLguBSudcURCWG1FuWzLJ634p4frH3tZIRyISdL05bPE3wDpgmpkVmtkXzex2M7vdm+VFYC9QADwG/P2AVRvmbrkwj/++8Rx2FlfxiYffZHNhhd8liUgEMb8GZliwYIHbsGGDL+v22+bCCr789EZKq+v5ztUzuOXCPMzM77JEJAyY2XvOuQVdPae9cz6Ym5vOn796ERdPyebeF7bxd0++R2lVvd9liUiYU6D7JD0pjsduXsB3r5nB2t1lXPGfa3huY6GGshORM6ZA91FUlPGlxRNZ+bXFTBmRwp2/+4Cbf/kuBaXVfpcmImFIgR4CJman8NvbLuDej85k06EKrnpgLfe9sJXK2ia/SxORMKJADxHRUcbnFk3gtW8u5bpzx/L4W/tZev9qHnl9j64FIyK9oqNcQlT+4Ur+bdVO1uwqIyslni8vncQN548jIVYnJIkMZT0d5aJAD3Eb9h/jP/6yi3V7y8lOjedzF+Zx4/njSUuK9bs0EfGBAj0CvLXnKD97bQ9rdx8lKS6a688dxxcuyiM3I8nv0kRkECnQI8i2I1X8fO1env/gCA64fMYIbjh/PBdNziIqSicniUQ6BXoEOlJRx5PrDvC7DYc4dqKRccOT+Mx54/jUglyyUuL9Lk9EBogCPYI1NLfwUn4xz7xzkHf3HSM22rh8xkiunT+GpdNGEBejA5lEIokCfYjYXVLNr989yPObjlB+opH0pFg+MncU187P5exx6bpejEgEUKAPMU0trbyx+yjPvX+Yv2wtpqG5lbzMJD4ydzTLZucwa/QwhbtImFKgD2HV9U28lF/MnzYdZt2eclodjBuexPLZOSybncO8sWq5i4QTBboAUF7TwMvbSliZX8ybBUdpbnWMTkvgqtk5XDFjJAvyhqvPXSTEKdDlFJW1Tfx1ewkr84tYs/sojc2tpMbHcPHUbC6dPoKl07LJ1NEyIiFHgS49OtHQzJsFR3l1Rymv7iiltLoBM5g/Np3LZozk0ukjmJ6Tqq4ZkRCgQJdea211bD1S5YV7CR8UVgKQMyyBxVOyuHhqNhdNziIjOc7nSkWGJgW6nLHSqnpW7yzl9V1lvLH7KFX1zZjB3DFpXDw1m8VTspk/Lp3YaPW9iwwGBboERUur44PCCtbuOsqa3WVsOlRBS6sjJT6GCyZlcrHXgh+fmex3qSIRS4EuA6Kyrol1e46yZvdR1uwqo/B4HRA4LPLiqVksnpLNBZMyGZagK0OKBIsCXQacc4795bWs2VXGml1lrNtbTm1jC9FRxvyx6Syeks3iqVnMHZNGjLpnRM6YAl0GXWNzK+8fPM7a3UdZu7uMzYcrcQ6GJcSwaHKg9b54ShZjh+vyvyJ9oUAX3x0/0cibe462978XVdYDkJeZ1B7uF0zKJFXdMyI9UqBLSHHOsafsBGt3l7F291HW7SmnrinQPXP2uPT2gJ+bm060rvEuchIFuoS0huYWNh6oYO3uMt4oOMoWr3smLTGWRZMz2wNeozOJKNAlzBw70cibBUfbW/Bt3TMTs5JZPCXQ/75wUiYp8TE+Vyoy+BToErYC3TM1rNkVCPi39x6jrqmFmCjj7PEZXDwli0unj2TGKF2aQIYGBbpEjIbmFt478OHRM/mHqwAYnZbApTNGcNn0kVwwKZOE2GifKxUZGAp0iVil1fW8tqOMV3aUsHb3UWobW0iMjWbR5CwumzGCy6aPYMSwBL/LFAkaBboMCfVNLbyz7xivbC/hle2lHK4InLk6Z0yaF+4jmT1GozVJeFOgy5DjnGNnSTWvbC/lle0lvH+oAucCXTNXzsrhqlk5nJuXobNWJewo0GXIK69p4NUdpfxlWwlrdpXR0NxKRlIsV8wcyVWzclg0OUv97hIW+h3oZrYM+C8gGvi5c+5HnZ7/HPDvwGFv0kPOuZ/3tEwFuviltrGZ13eW8dLWYl7dXkp1QzPJcdEsnT6Cq2blcMm0bJ2xKiGrp0A/7YG8ZhYNPAxcARQC683seefctk6z/tY595V+VysywJLiYlg+ZxTL54yisbmVdXvLeSm/mJe3lfDnzUXERUexaHImy2bncPmMkRqKT8JGb87MOA8ocM7tBTCzZ4GPA50DXSTsxMVEsWRqNkumZvP9T8xm48HjrMovZtW2Yr71hy1E2RbOmzCcZbNyuGp2DqPSEv0uWaRbp+1yMbNPAsucc1/yHt8EnN+xNe51ufwQKAN2Af/gnDvUxbJuBW4FGDdu3DkHDhwI0tsQCS7nHNuKqliVX8xLW4vZVVIDwLyx6SybncPy2TkayEN80a8+9F4GeiZQ45xrMLPbgOucc5f2tFz1oUs42VNWw0v5xazaWsxmb5zV6TmpLJ89imWzc5g6MkWHQ8qg6G+gXwDc65y7ynv8bQDn3A+7mT8aOOacS+tpuQp0CVeFx2tZtbWEVfnFrD9wDOdgQlYyy2bnsGxWDnNz0xTuMmD6G+gxBLpRLiNwFMt64LPOua0d5hnlnCvy7l8LfMs5t7Cn5SrQJRKUVtfz8rYSXsovZt2ecppbHaPTErjKC/cFecN1CWAJqmActng18ACBwxZ/6Zz7gZl9D9jgnHvezH4IfAxoBo4BX3bO7ehpmQp0iTQVtY28sr2UlfnFrNldRmNzK1kpcVwxM9DnvnBiJnExOpFJ+kcnFokMshMNzazeWcpL+cWs3lHKicYWhiXEcPmMkSybncPFU7N1IpOcEQW6iI/qm1p4Y/dRXtoaONa9sq6JpLhoLpk2gqtm60Qm6Zt+nVgkIv2TEBvN5TNHcvnMkTS1tPLO3mO8tLWIVVtL+POWwIlMi6dkcdXsHK6YMZKM5Di/S5YwpRa6iE9aWh3vHzzOyvxiXsov5nBFHdFRxsKJ3olMs3J06V85hbpcREKcc478w1W8tLWIlfnF7C07gRmcPS6DK2eO5JLpI5gyQse6iwJdJOzsLqnmpfxiVuYXs60oMCrTmPRElk7L5tLpI7hgUiZJceoxHYoU6CJhrKiyjtd2lrF6RylvFARGZYqLiWLhxEwumZbNJdNGkJelyxAMFQp0kQjR0NzC+n3HWb2zlNU7S9lbdgIInKm6eEoWF07K4oKJmaQl6aiZSKVAF4lQB8pP8NrOMl7dUcq7+45R19RClAWG3btwchaLJmWxIC9Dx7xHEAW6yBDQ2NzK+weP8+aect4qOMqmQxU0tzriYqJYMD6DCydlcm7ecM4am66AD2MKdJEhqKahmXf3lfNmQTlvFhxlR3E1AHHRUczJTWNBXgbn5Q3nnPEZpCfp2PdwoUAXEY6daOS9A8fZsP8Y6/cfY8vhSppaAr//U0emcK4X7meNTWdCZjJRuqhYSFKgi8gp6pta2HSowgv442w8cJzqhmYAUuNjmDs2jbm56ZyVm85ZY9PIGZag4+BDgE79F5FTJMRGs3BiJgsnZgKBM1cLSmv4oLCCDw5VsLmwksfW7KW5NdDoG5Eaz9zcdGaNHsaMUcOYOWoYuRmJasmHEAW6iAAQHWVMy0llWk4qn14wFgi04rcVVbHZC/hNhRW8sqOEtn/sU+JjmJaTyoxRqcwYFQj6aSNTSY5XtPhBXS4i0id1jS3sLKlme1EVO4qq2F4UuN/WXQOBs1onZiczeUQKk7IDt8kjUshKiVO3TT+py0VEgiYxLpp5Y9OZNza9fZpzjsLjdewormZncRUFpTXsKTvBb9cforaxpX2+YQkxTBqRwuTsFPKykhk3PIlxw5MYn5lEWmKswr6fFOgi0m9mxtjhSYwdnsQVM0e2T3fOUVRZz56yGi/ka9hTeoLXdpVR9l7hSctITYhhfGaSF/LJ7UE/Oj2RUWkJOna+FxToIjJgzIzR6YmMTk9k8ZTsk56rbWzm4LFaDpbXBn4eq+VAeS07iqp5eVtJ+yGVbYYnxzEqLYFRaYmMTk9oD/q2nyOHJRAbPbSH+FOgi4gvkuJimJ4zjOk5w055rqXVUVxVz8HyWo5U1FFUWceRynqKKuooPF7Lu/vKqapvPuV1w5PjyE6JZ8SweLJT4slO7XDr8DhSu3cU6CIScqKjjDHpiYxJT+x2npqGZoor6zhSUU9RZR1FlfUcrWmgtKqBspoG9h09QWl1A43Nrae8NjbayEyOJyM5joykWDKS4xieFNf+eHhyHBlJcQxPjiPde5wYGx3yfwQU6CISllLiY5g8IpXJI1K7ncc5R3VDM2XVDZRVN1Da/rOeYzWNHK9t4nhtI9uPVHGstpHKuia6O/AvPiaKjKQ4hiXGMCwhlmGJsQxLiCE1IbbTtFhSE2Lanx+WGHgcHzPw+wAU6CISscwsELQJsUzKTjnt/C2tjsq6Jo6daKSitpFjJxo5XtvIsRNN7Y+r65upqm+itLqePWXNVNU1UVXfTEtrz4eAx8dEkeqF/Q3nj+NLiycG6222U6CLiHiio4zhyYGulr5wzlHb2EJVfVMg8OuaqKpvoqquuX1aZV0TNQ3NVNc3k50aPyD1K9BFRPrJzEiOjyE5PoZRaf7VMbSP8RERiSAKdBGRCKFAFxGJEAp0EZEIoUAXEYkQCnQRkQihQBcRiRAKdBGRCOHbiEVmVgYcOMOXZwFHg1hOMIVqbaqr70K1NtXVN6FaF5xZbW5TsY0AAAbrSURBVOOdc9ldPeFboPeHmW3obggmv4Vqbaqr70K1NtXVN6FaFwS/NnW5iIhECAW6iEiECNdAf9TvAnoQqrWprr4L1dpUV9+Eal0Q5NrCsg9dREROFa4tdBER6USBLiISIcIu0M1smZntNLMCM7trkNc91sxWm9k2M9tqZl/zpt9rZofNbJN3u7rDa77t1brTzK4awNr2m9kWb/0bvGnDzexlM9vt/czwppuZrfDq2mxmZw9gXdM6bJdNZlZlZl/3Y5uZ2S/NrNTM8jtM6/M2MrNbvPl3m9ktA1TXv5vZDm/dfzSzdG96npnVddhuj3R4zTned6DAq73fIxp3U1ufP7tg/952U9dvO9S038w2edMHbZv1kBGD8z1zzoXNDYgG9gATgTjgA2DmIK5/FHC2dz8V2AXMBO4FvtnF/DO9GuOBCV7t0QNU234gq9O0fwPu8u7fBfzYu381sBIwYCHwziB+fsXAeD+2GXAxcDaQf6bbCBgO7PV+Znj3MwagriuBGO/+jzvUlddxvk7Leder1bzalw/QNuvTZzcQv7dd1dXp+f8A7hnsbdZDRgzK9yzcWujnAQXOub3OuUbgWeDjg7Vy51yRc26jd78a2A6M6eElHweedc41OOf2AQUE3sNg+TjwhHf/CeATHaY/6QLeBtLNbNQg1HMZsMc519MZwgO2zZxza4BjXayvL9voKuBl59wx59xx4GVgWbDrcs79xTnX7D18G8jtaRlebcOcc2+7QCI82eG9BLW2HnT32QX997anurxW9qeB3/S0jIHYZj1kxKB8z8It0McAhzo8LqTnQB0wZpYHzAfe8SZ9xfuX6Zdt/04xuPU64C9m9p6Z3epNG+mcK/LuFwMjfairo+s5+ZfM720Gfd9Gfmy7LxBoxbWZYGbvm9nrZrbYmzbGq2Ww6urLZzfY22wxUOKc291h2qBvs04ZMSjfs3AL9JBgZinAH4CvO+eqgJ8Bk4B5QBGBf/cG20XOubOB5cAdZnZxxye9Fohvx6iaWRzwMeB/vEmhsM1O4vc26oqZfQdoBp7xJhUB45xz84E7gV+b2bBBLivkPrtOPsPJDYdB32ZdZES7gfyehVugHwbGdnic600bNGYWS+CDesY59xyAc67EOdfinGsFHuPDLoJBq9c5d9j7WQr80auhpK0rxftZOth1dbAc2OicK/Hq9H2befq6jQatPjP7HPAR4AYvBPC6M8q9++8R6Jue6tXQsVtmIL9rff3sBnObxQB/A/y2Q72Dus26yggG6XsWboG+HphiZhO8Ft/1wPODtXKvb+4XwHbn3E86TO/Y/3wt0Lbn/XngejOLN7MJwBQCO2GCXVeymaW23SewQy3fW3/b3vFbgP/tUNfN3h72hUBlh38HB8pJrSa/t1kHfd1Gq4ArzSzD62q40psWVGa2DPgn4GPOudoO07PNLNq7P5HA9tnr1VZlZgu97+nNHd5LsGvr62c3mL+3lwM7nHPtXSmDuc26ywgG63vWnz26ftwI7BXeReCv7HcGed0XEfhXaTOwybtdDTwFbPGmPw+M6vCa73i17iQIRx10U9dEAkcOfABsbdsuQCbwCrAb+Csw3JtuwMNeXVuABQO83ZKBciCtw7RB32YE/qAUAU0E+iS/eCbbiECfdoF3+/wA1VVAoA+17Xv2iDfv33qf8SZgI/DRDstZQCBc9wAP4Z0JPgC19fmzC/bvbVd1edMfB27vNO+gbTO6z4hB+Z7p1H8RkQgRbl0uIiLSDQW6iEiEUKCLiEQIBbqISIRQoIuIRAgFusgZMLOlZvZ/ftch0pECXUQkQijQJaKZ2Y1m9q4FroP932YWbWY1ZvafFrhe9Stmlu3NO8/M3rYPr0Heds3qyWb2VzP7wMw2mtkkb/EpZvZ7C1y3/BnvLEER3yjQJWKZ2QzgOmCRc24e0ALcQODM1Q3OuVnA68C/eC95EviWc24ugbP22qY/AzzsnDsLuJDAGYoQuJLe1wlc73oisGjA35RID2L8LkBkAF0GnAOs9xrPiQQuitTKhxdvehp4zszSgHTn3Ove9CeA//GukTPGOfdHAOdcPYC3vHedd80QC4yOkwe8MfBvS6RrCnSJZAY84Zz79kkTze7uNN+ZXv+iocP9FvT7JD5Tl4tEsleAT5rZCGgf13E8ge/9J715Pgu84ZyrBI53GPzgJuB1Fxh1ptDMPuEtI97Mkgb1XYj0kloUErGcc9vM7LsERnKKInBlvjuAE8B53nOlBPrZIXBZ00e8wN4LfN6bfhPw32b2PW8ZnxrEtyHSa7raogw5ZlbjnEvxuw6RYFOXi4hIhFALXUQkQqiFLiISIRToIiIRQoEuIhIhFOgiIhFCgS4iEiH+P2S1yd97T9suAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weg6gDSL6T4I"
      },
      "source": [
        "So there you go, you can write a simple machine learning code in just 30 minutes using Python and Google Colab.\n"
      ]
    }
  ]
}